{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Raspbernetes","text":"<p>This repo is a declarative implementation of a Kubernetes cluster which follows GitOps princples. It's using the FluxCD.</p>"},{"location":"#mission","title":"Mission","text":"<p>The goal is to demonstrates how to implement enterprise-grade security, observability, and overall cluster config management using GitOps in a Kubernetes cluster.</p>"},{"location":"#story","title":"Story","text":"<p>This project ...</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/","title":"Magento Production Migration Guide","text":"<p>Reference document for migrating Magento production data from VPS into the Kubernetes cluster. Based on the auntalma.com.au migration (Feb 2026).</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#overview","title":"Overview","text":"<p>Purpose: This is a reusable guide for importing Magento production data into the Kubernetes cluster. Do NOT use this document as a progress tracker \u2014 it is a reference for future agents and operators to follow when performing a migration.</p> <p>Source: VPS at <code>web@103.21.131.100:22222</code> (SSH key: <code>~/.ssh/rog_laptop_key</code>) Target: MariaDB Galera cluster + Magento pods in <code>business-system</code> namespace Strategy: Test with <code>*.haydenagencies.com.au</code> subdomain first, then cutover to production domain</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#known-issues-solutions","title":"Known Issues &amp; Solutions","text":"<p>The following issues were discovered during the auntalma.com.au migration (Feb 2026). Each entry documents the symptom, root cause, and fix to prevent re-discovery.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#1-mariadb-galera-crashloopbackoff-after-operator-upgrade","title":"1. MariaDB Galera CrashLoopBackOff After Operator Upgrade","text":"<p>Symptom: All 3 Galera nodes in CrashLoopBackOff after mariadb-operator upgrade to v25.10.4.</p> <p>Root Cause: Renovate bumped the operator but the CRD field <code>automaticFailover</code> was renamed to <code>autoFailover</code> in the new version.</p> <p>Fix: Update <code>mariadb-cluster.yaml</code>: <pre><code># Before (broken)\ngalera:\n  primary:\n    automaticFailover: true\n\n# After (fixed)\ngalera:\n  primary:\n    autoFailover: true\n</code></pre></p> <p>Also update agent/initContainer images to match operator version: <pre><code>agent:\n  image: docker-registry3.mariadb.com/mariadb-operator/mariadb-operator:25.10.4\ninitContainer:\n  image: docker-registry3.mariadb.com/mariadb-operator/mariadb-operator:25.10.4\n</code></pre></p> <p>Resolution: Nuked PVCs and fresh-started since we were importing production data anyway.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#2-mariadb-121-incompatible-with-magento-247","title":"2. MariaDB 12.1 Incompatible with Magento 2.4.7","text":"<p>Symptom: <code>setup:upgrade</code> fails with: <pre><code>Current version of RDBMS is not supported. Used Version: 12.1.2-MariaDB-ubu2404.\nSupported versions: MySQL-8, MySQL-5.7, MariaDB-(10.2-10.11)\n</code></pre></p> <p>Root Cause: Renovate auto-upgraded <code>mariadb:10.11</code> to <code>mariadb:12.1</code>. Magento 2.4.7 only supports MariaDB 10.2-10.11.</p> <p>Fix: Downgrade MariaDB to 10.11 LTS. Pinned <code>image: mariadb:10.11</code> in both <code>auntalma/mariadb-cluster.yaml</code> and <code>hayden/mariadb-cluster.yaml</code>. Added Renovate <code>allowedVersions: \"/^10\\\\.11\\\\./\"</code> rule in <code>.github/renovate.json5</code> to prevent future auto-upgrades.</p> <p>Downgrade procedure (12.1 data files are not backward-compatible): 1. Delete the 3 Galera PVCs (<code>kubectl delete pvc -n business-system -l app.kubernetes.io/instance=&lt;site&gt;-mariadb</code>) 2. Let the mariadb-operator recreate the cluster on 10.11 3. Re-import production data</p> <p>This is viable because the migration is a fresh import anyway. The alternative (third-party <code>amadeco/module-db-override</code> composer module) adds an unnecessary dependency.</p> <p>Note: MariaDB 10.11 uses <code>mysql</code> CLI binary (not <code>mariadb</code> as in 12.1). All <code>kubectl exec</code> commands in this doc use <code>mysql</code> accordingly.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#3-database-import-kubectl-exec-pipe-timeout","title":"3. Database Import: kubectl exec Pipe Timeout","text":"<p>Symptom: Piping SQL dump (161 MB) through <code>kubectl exec</code> times out: <pre><code>error: unable to upgrade connection: i/o timeout\n</code></pre></p> <p>Root Cause: Streaming large files through the K8s API server is unreliable.</p> <p>Fix: Copy the dump into the pod first, then import locally: <pre><code># Step 1: Copy dump into pod\nkubectl cp /tmp/auntalma_dump.sql business-system/auntalma-mariadb-0:/tmp/auntalma_dump.sql -c mariadb\n\n# Step 2: Import locally inside the container (redirect must happen inside the container)\nkubectl exec auntalma-mariadb-0 -n business-system -c mariadb -- \\\n  bash -c 'mariadb -u root -p\"$MARIADB_ROOT_PASSWORD\" magento &lt; /tmp/auntalma_dump.sql'\n</code></pre></p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#4-database-import-lock-tables-error-with-galera","title":"4. Database Import: LOCK TABLES Error with Galera","text":"<p>Symptom: Import fails with: <pre><code>ERROR 1100 (HY000): Table 'rating_option_vote' was not locked with LOCK TABLES\n</code></pre></p> <p>Root Cause: mysqldump includes <code>LOCK TABLES</code> / <code>UNLOCK TABLES</code> statements. Galera doesn't support <code>LOCK TABLES</code> in normal transaction flow.</p> <p>Fix: Strip lock statements from the dump before importing: <pre><code># Option A: Pipe through sed\ncat /tmp/auntalma_dump.sql | sed '/^LOCK TABLES/d; /^UNLOCK TABLES/d' | \\\n  kubectl exec -i auntalma-mariadb-0 -n business-system -c mariadb -- \\\n  mariadb -u root -p\"${ROOT_PW}\" magento\n\n# Option B: Clean the file first\nsed -i '/^LOCK TABLES/d; /^UNLOCK TABLES/d' /tmp/auntalma_dump.sql\n</code></pre></p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#5-mariadb-cli-binary-name-change","title":"5. MariaDB CLI Binary Name Change","text":"<p>Symptom: <code>mysql</code> command not found in MariaDB 12.1 container.</p> <p>Root Cause: MariaDB 12.1 renamed the CLI binary from <code>mysql</code> to <code>mariadb</code>.</p> <p>Note: Not relevant when using MariaDB 10.11 (Issue #2). The <code>mysql</code> binary works on 10.11. Commands in this doc use <code>mysql</code> throughout.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#6-mage_run_code-mismatch","title":"6. MAGE_RUN_CODE Mismatch","text":"<p>Symptom: <code>setup:upgrade</code> fails with: <pre><code>website with code auntalma that was requested wasn't found\n</code></pre></p> <p>Root Cause: The HelmRelease had <code>MAGE_RUN_CODE=auntalma</code> but the production database uses <code>base</code> as the website code for Aunt Alma.</p> <p>Discovery: Query the production DB to find actual website codes: <pre><code>SELECT website_id, code, name FROM store_website;\n-- Results: admin(0), base(1=Aunt Alma), drop_drape_second_site(2)\n\nSELECT store_id, code, website_id, name FROM store;\n-- Results: admin(0), default(1=Aunt Alma), drop_drape_store_view(2)\n</code></pre></p> <p>Fix: Update <code>MAGE_RUN_CODE</code> in helmrelease.yaml to match the production DB: <pre><code>- name: MAGE_RUN_CODE\n  value: base  # NOT \"auntalma\" - must match store_website.code\n</code></pre></p> <p>Lesson: Always check <code>store_website</code> and <code>store</code> tables after import to find the correct codes.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#7-alpinemusl-dns-resolution-failure-local-domains","title":"7. Alpine/musl DNS Resolution Failure (.local domains)","text":"<p>Symptom: PHP can't connect to services: <pre><code>php_network_getaddresses: getaddrinfo for auntalma-mariadb-primary.business-system.svc.cluster.local failed: Try again\n</code></pre></p> <p>But <code>nslookup</code> from the same pod works fine. Short hostnames also work: <pre><code>gethostbyname('auntalma-mariadb-primary');  // Returns 10.110.12.97 (works!)\ngethostbyname('auntalma-mariadb-primary.business-system.svc.cluster.local');  // Returns the hostname itself (FAILS!)\n</code></pre></p> <p>Root Cause: Alpine Linux uses musl libc, which has a known bug with <code>.local</code> TLD. musl treats <code>.local</code> as mDNS and fails to resolve via standard DNS.</p> <p>Fix: Use short service names instead of FQDNs in helmrelease.yaml env vars. Since pods and services are in the same namespace (<code>business-system</code>), the search domain automatically appends the rest: <pre><code># Before (broken on Alpine/musl)\n- name: MAGENTO_DB_HOST\n  value: auntalma-mariadb-primary.business-system.svc.cluster.local\n\n# After (works on Alpine/musl)\n- name: MAGENTO_DB_HOST\n  value: auntalma-mariadb-primary\n</code></pre></p> <p>Apply to ALL service hostnames: MariaDB, Redis, Elasticsearch, RabbitMQ.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#8-stale-elasticsearch-config-in-database","title":"8. Stale Elasticsearch Config in Database","text":"<p>Symptom: <code>setup:upgrade</code> fails with: <pre><code>Could not validate a connection to Elasticsearch. No alive nodes found in your cluster\n</code></pre></p> <p>Root Cause: The production database has ES config in <code>core_config_data</code> pointing to the old Elastic Cloud instance (e.g., <code>magento-2-43eba1.es.ap-southeast-2.aws.found.io:9243</code> with auth enabled). This DB-stored config overrides whatever is in <code>env.php</code>.</p> <p>Fix: Update ES config in the database: <pre><code>UPDATE core_config_data SET value = 'auntalma-elasticsearch-es-http'\n  WHERE path = 'catalog/search/elasticsearch7_server_hostname';\nUPDATE core_config_data SET value = '9200'\n  WHERE path = 'catalog/search/elasticsearch7_server_port';\nUPDATE core_config_data SET value = '0'\n  WHERE path = 'catalog/search/elasticsearch7_enable_auth';\nUPDATE core_config_data SET value = ''\n  WHERE path LIKE 'catalog/search/elasticsearch7_username';\nUPDATE core_config_data SET value = ''\n  WHERE path LIKE 'catalog/search/elasticsearch7_password';\n</code></pre></p> <p>IMPORTANT: Also flush Redis after changing DB config (see next issue).</p> <p>Note: <code>env.php</code> sets ES config via <code>getenv('MAGENTO_ES_HOST')</code> under the <code>system.default.catalog.search</code> key, but the DB-stored <code>core_config_data</code> takes precedence for already-configured instances.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#9-redis-caching-stale-config","title":"9. Redis Caching Stale Config","text":"<p>Symptom: After updating ES config in the DB, <code>setup:upgrade</code> still fails with the old Elastic Cloud hostname.</p> <p>Root Cause: Magento caches its configuration in Redis. The stale ES config is served from Redis cache, not from the DB.</p> <p>Fix: Flush the relevant Redis databases: <pre><code># From the PHP pod:\nkubectl exec $POD -n business-system -c php -- php -r \"\n\\$r = new Redis();\n\\$r-&gt;connect('auntalma-redis-master', 6379);\n\\$r-&gt;select(6); \\$r-&gt;flushDB();  // DB 6 = Magento cache\n\\$r-&gt;select(1); \\$r-&gt;flushDB();  // DB 1 = Page cache\necho 'Flushed Redis DBs 6 and 1';\n\"\n</code></pre></p> <p>Lesson: Always flush Redis after changing any <code>core_config_data</code> values.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#10-trigger-privilege-error-with-galera-binary-logging","title":"10. Trigger Privilege Error with Galera Binary Logging","text":"<p>Symptom: <code>setup:upgrade</code> fails during schema updates: <pre><code>SQLSTATE[HY000]: General error: 1419 You do not have the SUPER privilege and binary logging is enabled\n(you *might* want to use the less safe log_bin_trust_function_creators variable),\nquery was: DROP TRIGGER IF EXISTS `trg_catalog_category_entity_after_insert`\n</code></pre></p> <p>Root Cause: Galera uses <code>binlog_format=ROW</code> for replication. Creating/dropping triggers requires SUPER privilege or <code>log_bin_trust_function_creators=1</code>.</p> <p>Fix: Add to <code>mariadb-cluster.yaml</code> myCnf (permanent): <pre><code>myCnf: |\n  [mariadb]\n  ...\n  log_bin_trust_function_creators=1\n</code></pre></p> <p>Also set at runtime (immediate, doesn't require MariaDB restart): <pre><code>kubectl exec auntalma-mariadb-0 -n business-system -c mariadb -- \\\n  mariadb -u root -p\"${ROOT_PW}\" -e \"SET GLOBAL log_bin_trust_function_creators=1;\"\n</code></pre></p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#11-flux-rollback-loop-chicken-and-egg","title":"11. Flux Rollback Loop (Chicken-and-Egg)","text":"<p>Symptom: Flux deploys new pods with updated env vars, but Magento fails health checks (because <code>setup:upgrade</code> hasn't run), so Flux rolls back to the old template.</p> <p>Root Cause: Magento can't serve HTTP until the DB schema is up to date. But the new pods need to serve HTTP to pass health checks. And you can't run <code>setup:upgrade</code> on old pods with new env vars easily.</p> <p>Workaround: Run <code>setup:upgrade</code> on the current (old) pod with env var overrides: <pre><code>kubectl exec $POD -n business-system -c php -- bash -c \"\nexport MAGENTO_DB_HOST=auntalma-mariadb-primary\nexport MAGENTO_SESSION_REDIS_HOST=auntalma-redis-master\nexport MAGENTO_CACHE_REDIS_HOST=auntalma-redis-master\nexport MAGENTO_PAGE_CACHE_REDIS_HOST=auntalma-redis-master\nexport MAGENTO_ES_HOST=auntalma-elasticsearch-es-http\nexport MAGENTO_AMQP_HOST=auntalma-rabbitmq\nexport MAGE_RUN_CODE=base\ncd /var/www/html &amp;&amp; php bin/magento setup:upgrade\n\"\n</code></pre></p> <p>Once setup:upgrade completes, the schema is updated in the DB, and new pods deployed by Flux will be able to serve.</p> <p>Alternative: Suspend the HelmRelease temporarily: <pre><code>flux suspend helmrelease auntalma -n business-system\n# ... run setup:upgrade ...\nflux resume helmrelease auntalma -n business-system\n</code></pre></p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#12-multi-store-discovery","title":"12. Multi-Store Discovery","text":"<p>Finding: The production database contains multiple stores: - website 1 (code: <code>base</code>): auntalma.com.au - website 2 (code: <code>drop_drape_second_site</code>): magento.toemass.com / dropdrape.com.au</p> <p>The K8s deployment is single-store (<code>MAGE_RUN_CODE=base</code>, nginx <code>server_name _</code>). Multi-store requires: - Nginx hostname-to-store-code mapping - env.php changes to read store code from HTTP headers - Separate DNS entries and HTTPRoutes - Deferred to follow-up task</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#13-setupupgrade-requires-dicompile-afterwards","title":"13. setup:upgrade Requires di:compile Afterwards","text":"<p>Symptom: After <code>setup:upgrade</code> succeeds, <code>indexer:reindex</code> fails with: <pre><code>There are no commands defined in the \"indexer\" namespace.\n</code></pre></p> <p>Root Cause: <code>setup:upgrade</code> clears the generated DI code and outputs: <pre><code>Please re-run Magento compile command. Use the command \"setup:di:compile\"\n</code></pre></p> <p>Without compiled DI, most Magento CLI commands are unavailable.</p> <p>Fix: Always run <code>setup:di:compile</code> after <code>setup:upgrade</code>: <pre><code>php bin/magento setup:di:compile\n</code></pre></p> <p>Note: <code>di:compile</code> takes several minutes and is CPU-intensive. It regenerates all interceptors/proxies/factories in <code>generated/code/</code>.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#14-flux-pods-replaced-mid-migration","title":"14. Flux Pods Replaced Mid-Migration","text":"<p>Symptom: While running manual setup commands via <code>kubectl exec</code>, the target pod disappears: <pre><code>Error from server (NotFound): pods \"auntalma-xxxxx\" not found\n</code></pre></p> <p>Root Cause: Flux HelmRelease reconciliation replaces pods on every cycle (rollback, upgrade, or config change). Your exec session is killed mid-operation.</p> <p>Workaround: Suspend the HelmRelease before starting manual migration steps: <pre><code>flux suspend helmrelease auntalma -n business-system\n# ... run all setup commands ...\nflux resume helmrelease auntalma -n business-system\n</code></pre></p> <p>Alternative: Work fast and re-apply patches each time pods rotate. The DB changes (schema, config) persist across pod restarts \u2014 only the di.xml patch and generated code are lost.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#15-envphp-missing-installdate-key","title":"15. env.php Missing <code>install.date</code> Key","text":"<p>Symptom: After <code>di:compile</code> succeeds, <code>indexer:reindex</code> fails with: <pre><code>There are no commands defined in the \"indexer\" namespace.\n</code></pre></p> <p><code>setup:db:status</code> reports: \"No information is available: the Magento application is not installed.\"</p> <p>Only base commands (setup, admin, module, etc.) appear in <code>php bin/magento list</code> \u2014 no <code>indexer</code>, <code>cache</code>, <code>cron</code>, <code>catalog</code>, etc.</p> <p>Root Cause: The env.php ConfigMap was missing the <code>install</code> key. Magento checks <code>install/date</code> in <code>env.php</code> to determine if the application is installed. Without it, most module CLI commands are not registered.</p> <p>Fix: Add to the env.php ConfigMap: <pre><code>'install' =&gt; [\n    'date' =&gt; 'Sat, 01 Jan 2022 00:00:00 +0000',\n],\n</code></pre></p> <p>Important: env.php is a read-only ConfigMap subPath mount \u2014 you CANNOT modify it in-container. You must update the ConfigMap resource, then restart the pod.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#16-pubstatic-emptydir-no-static-content-on-pod-start","title":"16. <code>pub/static</code> EmptyDir \u2014 No Static Content on Pod Start","text":"<p>Symptom: Storefront returns Magento 404 page. Exception log shows: <pre><code>Unable to retrieve deployment version of static files from the file system.\n</code></pre></p> <p><code>pub/static/deployed_version.txt</code> does not exist. <code>pub/static/</code> is empty.</p> <p>Root Cause: The HelmRelease mounts <code>pub/static</code> as an <code>emptyDir</code> volume (shared between PHP and nginx containers for static file serving). This hides the Docker image's baked-in static content with an empty directory. There is no init container to copy or regenerate the static files.</p> <p>Fix needed: Add an init container to the HelmRelease that copies pre-built static content from the image to the emptyDir. The key is using <code>advancedMounts</code> so the init container sees the image's original files (not the empty mount):</p> <pre><code># In HelmRelease values:\ncontrollers:\n  &lt;site&gt;:\n    initContainers:\n      copy-static:\n        image:\n          repository: ghcr.io/hayden-agencies/magento2-makergroup\n          tag: latest\n        command: [\"/bin/sh\"]\n        args:\n          - -c\n          - |\n            echo \"[init] Copying static content to shared volume...\"\n            if [ -d /var/www/html/pub/static ] &amp;&amp; [ \"$(ls -A /var/www/html/pub/static 2&gt;/dev/null)\" ]; then\n              cp -a /var/www/html/pub/static/. /shared-static/\n              echo \"[init] Done ($(find /shared-static -type f | wc -l) files)\"\n            else\n              echo \"[init] WARNING: No static content in image \u2014 need to add SCD to Dockerfile\"\n            fi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: false\n          capabilities: { drop: [\"ALL\"] }\n        resources:\n          requests: { cpu: 100m, memory: 256Mi }\n          limits: { cpu: \"2\", memory: 1Gi }\n\npersistence:\n  docroot:\n    type: emptyDir\n    advancedMounts:          # NOT globalMounts \u2014 critical!\n      &lt;site&gt;:\n        copy-static:\n          - path: /shared-static    # Different path so image content is visible\n        php:\n          - path: /var/www/html/pub/static\n        nginx:\n          - path: /var/www/html/pub/static\n</code></pre> <p>Why <code>advancedMounts</code>? With <code>globalMounts</code>, the emptyDir mounts at <code>/var/www/html/pub/static</code> in the init container too \u2014 hiding the image's static files. With <code>advancedMounts</code>, the init container mounts at <code>/shared-static</code> (the emptyDir) while its <code>/var/www/html/pub/static</code> remains the image's original content.</p> <p>Pre-requisite: The Docker image must have static content baked in. Verify: <pre><code>docker run --rm --entrypoint /bin/sh ghcr.io/hayden-agencies/magento2-makergroup:latest \\\n  -c \"ls /var/www/html/pub/static/ | head -10\"\n</code></pre> If empty, you need to add <code>setup:static-content:deploy</code> to the Dockerfile build phase.</p> <p>For immediate manual testing (lost on pod restart): <pre><code>kubectl exec $POD -n business-system -c php -- bash -c '\ncd /var/www/html &amp;&amp; php bin/magento setup:static-content:deploy en_US -f\n'\n</code></pre></p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#17-en_au-locale-dockerfile-deploys-wrong-locale","title":"17. <code>en_AU</code> Locale \u2014 Dockerfile Deploys Wrong Locale","text":"<p>Symptom: All <code>en_AU</code> static asset URLs return 404. Only <code>en_US</code> files exist under <code>pub/static/frontend/</code>. Pages load without CSS/JS.</p> <p>Root Cause: <code>Dockerfile.prod</code> line 77 deploys <code>en_US</code> for frontend themes with a misleading comment: \"en_AU falls back to en_US at runtime\". This is wrong \u2014 Magento does NOT fall back for static files. The HTML URLs are hardcoded to the store's configured locale (<code>en_AU</code>), and missing files return 404.</p> <p>Note: The <code>magento/language-en_au</code> composer package is NOT required for SCD. That package only provides UI string translations (csv files). SCD locale validation uses the ICU library + Magento's hardcoded allowlist (<code>Magento\\Framework\\Locale\\Config::$_allowedLocales</code>), which already includes <code>en_AU</code>. The <code>php:8.3-fpm-alpine</code> image installs <code>intl</code> (ICU) so <code>en_AU</code> is a valid SCD locale out of the box.</p> <p>Fix (permanent \u2014 update Docker image): 1. Change <code>Dockerfile.prod</code> frontend SCD from <code>en_US</code> to <code>en_AU</code> (line 77, matching <code>deploy.php</code>)</p> <p>Fix (temporary \u2014 in running pod, lost on restart): <pre><code># Copy en_US static files to en_AU directory\nkubectl exec $POD -n business-system -c php -- bash -c \\\n  \"cp -a /var/www/html/pub/static/frontend/Rival/auntalma/en_US/* \\\n   /var/www/html/pub/static/frontend/Rival/auntalma/en_AU/\"\n</code></pre></p> <p>See also Issue #42 for the related CSS minification problem.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#18-cilium-clusterip-routing-broken-on-some-nodes","title":"18. Cilium ClusterIP Routing Broken on Some Nodes","text":"<p>Symptom: MariaDB connection times out even when using the ClusterIP directly: <pre><code>SQLSTATE[HY000] [2002] Operation timed out\n</code></pre></p> <p>Pod-to-pod direct IP works, but ClusterIP does not. Affects specific worker nodes (worker2/worker3 in our case). Other services (Redis) work fine via ClusterIP from the same pod. The worker2 pod was persistently crashlooping (1/2 Ready, 11+ restarts) while worker1 was fine.</p> <p>Root Cause: Cilium's eBPF service routing on the affected node doesn't have the correct endpoint map for the MariaDB service. This is a node-level Cilium issue, not a DNS or Magento problem.</p> <p>Workaround: Use pod IPs or ClusterIPs directly to bypass DNS entirely. Get all IPs up front: <pre><code># Get all service ClusterIPs\nkubectl get svc -n business-system \\\n  &lt;site&gt;-redis-master &lt;site&gt;-mariadb-primary \\\n  &lt;site&gt;-elasticsearch-es-http &lt;site&gt;-rabbitmq \\\n  -o jsonpath='{range .items[*]}{.metadata.name}={.spec.clusterIP}{\"\\n\"}{end}'\n\n# If ClusterIP also fails, fall back to pod IP\nDB_POD_IP=$(kubectl get pod &lt;site&gt;-mariadb-0 -n business-system -o jsonpath='{.status.podIP}')\n</code></pre></p> <p>Diagnosis: <pre><code># From the affected pod \u2014 ClusterIP fails:\necho quit | nc -w 3 &lt;ClusterIP&gt; 3306    # TCP FAIL\n\n# Direct pod IP works:\necho quit | nc -w 3 &lt;PodIP&gt; 3306        # TCP OK\n</code></pre></p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#19-flux-rollback-loop-helmrelease-stuck-at-418-revisions","title":"19. Flux Rollback Loop \u2014 HelmRelease Stuck at 418+ Revisions","text":"<p>Symptom: <code>flux get helmrelease auntalma</code> shows: <pre><code>Helm rollback to previous release business-system/auntalma.v4 ... failed: context deadline exceeded\n</code></pre></p> <p><code>helm history auntalma -n business-system</code> shows hundreds of failed revisions, all rolling back to v4 (the pre-migration release).</p> <p>Root Cause: Cascading chicken-and-egg: 1. Flux upgrades the HelmRelease with new env vars/config 2. New pods start but can't pass health checks (Magento needs <code>setup:upgrade</code> first, static content is empty, etc.) 3. Helm timeout \u2192 Flux rolls back to v4 (old working config... which also doesn't work with imported DB) 4. Repeat every 15 minutes</p> <p>Status: The actual DB migration is done (setup:upgrade, di:compile, indexer:reindex all succeeded). But new pods from Flux can't serve because: - <code>pub/static</code> emptyDir is empty (no init container \u2014 Issue #16) - MariaDB 12 di.xml patch isn't baked in (Issue #2) - Pods on some workers can't reach MariaDB via ClusterIP (Issue #18)</p> <p>Fix plan: 1. Pin MariaDB to 10.11 OR bake di.xml patch into Docker image 2. Add init container for static content deploy to HelmRelease 3. Fix the <code>en_AU</code> locale (add to Docker image or configure stores for <code>en_US</code>) 4. Once those are in place, reset Helm release history and let Flux reconcile cleanly</p> <p>Cleaning up Helm history (after fixing all blockers): <pre><code># Option A: Uninstall and let Flux reinstall\nflux suspend helmrelease &lt;site&gt; -n business-system\nhelm uninstall &lt;site&gt; -n business-system\nflux resume helmrelease &lt;site&gt; -n business-system\n# Flux will do a fresh install with revision 1\n\n# Option B: If you need to keep the release\nhelm rollback &lt;site&gt; 4 -n business-system   # Roll back to last known good\n</code></pre></p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#20-flux-reverts-manually-applied-configmaps","title":"20. Flux Reverts Manually-Applied ConfigMaps","text":"<p>Symptom: You <code>kubectl apply</code> a ConfigMap fix (e.g., adding <code>install.date</code> to env.php), restart pods, confirm it works \u2014 then Flux reconciles and the fix disappears.</p> <p>Root Cause: Flux Kustomization reconciles all manifests from the OCI source every interval. If your change isn't committed and pushed to git (and built into the OCI image), Flux overwrites it with the old version.</p> <p>Lesson: <code>kubectl apply</code> is only a temporary hotfix. You MUST commit, push, and wait for the OCI build before Flux will persist the change. If Flux is suspended, the manual apply sticks until you resume.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#21-kubectl-set-env-as-emergency-deployment-patch","title":"21. <code>kubectl set env</code> as Emergency Deployment Patch","text":"<p>Symptom: Flux is in a rollback loop. Your git changes (short hostnames, MAGE_RUN_CODE) are committed but Flux keeps rolling back to the old Helm release (v4). New pods never get the correct env vars.</p> <p>Workaround: Bypass Helm/Flux entirely by patching the Deployment directly: <pre><code># Suspend Flux first\nflux suspend helmrelease &lt;site&gt; -n business-system\n\n# Patch all env vars in one command\nHTTPS_PROXY=socks5://127.0.0.1:1234 kubectl set env deployment/&lt;site&gt; -n business-system -c php \\\n  MAGE_RUN_CODE=base \\\n  MAGENTO_DB_HOST=&lt;site&gt;-mariadb-primary \\\n  MAGENTO_SESSION_REDIS_HOST=&lt;site&gt;-redis-master \\\n  MAGENTO_CACHE_REDIS_HOST=&lt;site&gt;-redis-master \\\n  MAGENTO_PAGE_CACHE_REDIS_HOST=&lt;site&gt;-redis-master \\\n  MAGENTO_ES_HOST=&lt;site&gt;-elasticsearch-es-http \\\n  MAGENTO_AMQP_HOST=&lt;site&gt;-rabbitmq\n</code></pre></p> <p>This triggers a new rollout with the correct env vars. Pods restart automatically.</p> <p>Important: This is a manual override \u2014 when you resume Flux, it will revert to whatever the HelmRelease specifies.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#22-configmap-subpath-mounts-dont-auto-update","title":"22. ConfigMap SubPath Mounts Don't Auto-Update","text":"<p>Symptom: You update a ConfigMap (<code>kubectl apply</code>), but the running pod still has the old content.</p> <p>Root Cause: ConfigMap volumes mounted via <code>subPath</code> are a one-time copy at pod creation. Unlike regular ConfigMap mounts (which update via symlink), subPath mounts never update.</p> <p>Fix: Restart the pod after updating the ConfigMap: <pre><code>kubectl rollout restart deployment/&lt;site&gt; -n business-system\n</code></pre></p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#23-dns-flaky-inside-bash-c-even-with-short-hostnames","title":"23. DNS Flaky Inside <code>bash -c</code> Even With Short Hostnames","text":"<p>Symptom: <code>php -r \"echo gethostbyname('auntalma-redis-master');\"</code> resolves fine when run directly via <code>kubectl exec</code>. But the same hostname fails inside <code>bash -c '...'</code>: <pre><code>php_network_getaddresses: getaddrinfo for auntalma-redis-master failed: Try again\n</code></pre></p> <p>Root Cause: Intermittent DNS resolution failure in Alpine/musl. Rapid sequential DNS lookups from shell subprocesses seem to trigger it. Not 100% reproducible \u2014 works sometimes, fails other times.</p> <p>Workaround: For migration commands, resolve all service IPs up front and use them directly: <pre><code># Get IPs before exec-ing into the pod\nREDIS_IP=$(kubectl get svc &lt;site&gt;-redis-master -n business-system -o jsonpath='{.spec.clusterIP}')\nDB_IP=$(kubectl get pod &lt;site&gt;-mariadb-0 -n business-system -o jsonpath='{.status.podIP}')\nES_IP=$(kubectl get svc &lt;site&gt;-elasticsearch-es-http -n business-system -o jsonpath='{.spec.clusterIP}')\nAMQP_IP=$(kubectl get svc &lt;site&gt;-rabbitmq -n business-system -o jsonpath='{.spec.clusterIP}')\n\n# Then use IPs in the exec command\nkubectl exec $POD -n business-system -c php -- bash -c \"\nexport MAGENTO_DB_HOST=$DB_IP\nexport MAGENTO_SESSION_REDIS_HOST=$REDIS_IP\n...\"\n</code></pre></p> <p>Note: Use pod IP for MariaDB if ClusterIP also fails (Issue #18).</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#24-httproute-backend-service-name-mismatch","title":"24. HTTPRoute Backend Service Name Mismatch","text":"<p>Symptom: Storefront returns <code>000</code> (connection refused). <code>curl -sk https://auntalma.haydenagencies.com.au/</code> gets no response at all.</p> <p>Root Cause: HTTPRoute <code>backendRefs</code> referenced <code>auntalma-app</code> but the bjw-s app-template creates the service as just <code>auntalma</code>. Envoy Gateway reported <code>BackendNotFound</code>: <pre><code>Failed to process route rule 0 backendRef 0: service business-system/auntalma-app not found.\n</code></pre></p> <p>Fix: Update <code>httproute.yaml</code> and <code>httproute-admin.yaml</code>: <pre><code># Before (broken)\nbackendRefs:\n  - name: auntalma-app\n    port: 8080\n\n# After (fixed)\nbackendRefs:\n  - name: auntalma\n    port: 8080\n</code></pre></p> <p>How to verify: Check HTTPRoute status \u2014 <code>ResolvedRefs</code> should be <code>True</code>: <pre><code>kubectl get httproute auntalma -n business-system -o jsonpath='{.status.parents[0].conditions}' | python3 -m json.tool\n</code></pre></p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#25-missing-external-dns-annotations-on-httproute","title":"25. Missing external-dns Annotations on HTTPRoute","text":"<p>Symptom: DNS doesn't resolve at all for <code>auntalma.haydenagencies.com.au</code>. No CNAME record in Cloudflare.</p> <p>Root Cause: External-dns is configured with <code>--annotation-filter=external-dns.alpha.kubernetes.io/external=true</code> and <code>--source=gateway-httproute</code>. HTTPRoutes without this annotation are ignored. Every other working route (chatwoot, odoo, grafana, etc.) has it.</p> <p>Fix: Add annotations to the HTTPRoute (only needed on the primary route, not the admin route): <pre><code>metadata:\n  name: auntalma\n  namespace: business-system\n  annotations:\n    external-dns.alpha.kubernetes.io/external: \"true\"\n    external-dns.alpha.kubernetes.io/target: external.haydenagencies.com.au\n</code></pre></p> <p>The <code>target</code> annotation tells external-dns to create a CNAME pointing to <code>external.haydenagencies.com.au</code> (which is the Cloudflare-proxied endpoint that routes through the cloudflare tunnel to envoy-external).</p> <p>Verification: Check external-dns logs for the record creation: <pre><code>kubectl logs -n network-system -l app.kubernetes.io/name=external-dns --tail=10\n# Should see: Changing record. action=CREATE record=auntalma.haydenagencies.com.au\n</code></pre></p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#26-networkpolicy-blocking-envoy-gateway-traffic","title":"26. NetworkPolicy Blocking Envoy Gateway Traffic","text":"<p>Symptom: Storefront returns <code>503</code> with <code>upstream connect error or disconnect/reset before headers. reset reason: connection timeout</code>. Envoy access log shows the request reaching the correct backend IP but timing out after 10 seconds: <pre><code>\"GET / HTTP/2\" 503 UF 0 91 10001 ... \"auntalma.haydenagencies.com.au\" \"10.244.9.122:8080\"\n</code></pre></p> <p>Meanwhile, the backend pod is healthy \u2014 <code>health_check.php</code> returns 200 when tested via <code>localhost</code> inside the pod.</p> <p>Root Cause: The <code>auntalma-app</code> NetworkPolicy allowed ingress from <code>envoy-gateway-system</code> namespace, but the Envoy Gateway proxy pods are actually in the <code>network-system</code> namespace: <pre><code># WRONG \u2014 envoy proxy pods are NOT in this namespace\n- namespaceSelector:\n    matchLabels:\n      kubernetes.io/metadata.name: envoy-gateway-system\n</code></pre></p> <p>Fix: Update <code>networkpolicy.yaml</code> to target the correct namespace and pods: <pre><code># CORRECT \u2014 match envoy proxy pods in network-system\n- from:\n    - namespaceSelector:\n        matchLabels:\n          kubernetes.io/metadata.name: network-system\n      podSelector:\n        matchLabels:\n          gateway.envoyproxy.io/owning-gateway-name: envoy-external\n  ports:\n    - port: 8080\n      protocol: TCP\n</code></pre></p> <p>Key insight: In this cluster, the Envoy Gateway controller runs in <code>envoy-gateway-system</code>, but the data-plane proxy pods (the ones that actually forward traffic) run in <code>network-system</code> alongside the Gateway resource. The label <code>gateway.envoyproxy.io/owning-gateway-name: envoy-external</code> specifically targets the proxy pods.</p> <p>How to debug: Check which namespace the envoy proxy pods are in: <pre><code>kubectl get pods -A -l gateway.envoyproxy.io/owning-gateway-name=envoy-external -o wide\n</code></pre></p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#27-networkpolicy-cilium-dns-proxy-breaks-alpine-musl-libc-dns","title":"27. NetworkPolicy + Cilium DNS Proxy Breaks Alpine musl libc DNS","text":"<p>Symptom: PHP-FPM health check times out \u2192 nginx container enters CrashLoopBackOff (139+ restarts). Magento can't connect to Redis/MariaDB: <pre><code>php_network_getaddresses: getaddrinfo for auntalma-redis-master failed: Try again\n</code></pre></p> <p>DNS works via <code>nslookup</code> and PHP's <code>dns_get_record()</code>, but fails via <code>gethostbyname()</code> and <code>getaddrinfo()</code> (used by PDO, fsockopen, Redis).</p> <p>Root Cause: Three-way incompatibility: 1. Alpine musl libc sends A and AAAA DNS queries simultaneously on the same UDP socket 2. Cilium transparent DNS proxy (<code>enable-l7-proxy: true</code>, <code>dnsproxy-enable-transparent-mode: true</code>) intercepts DNS when ANY NetworkPolicy (K8s or Cilium) is applied to a pod 3. The proxy re-orders or delays responses, causing musl's parallel query logic to fail intermittently</p> <p>Key findings: - Without any NetworkPolicy: DNS works 100% reliably - With K8s NetworkPolicy (even ingress-only, no egress rules): DNS fails intermittently - With CiliumNetworkPolicy (with or without <code>dns</code> L7 rules): DNS also fails - Non-Alpine pods (e.g., Chatwoot/Ruby) with the same NetworkPolicy pattern work fine - TCP DNS to kube-dns ClusterIP is blocked even when explicitly allowed (Cilium can't match service VIPs to pod selectors after DNAT)</p> <p>Tested and failed: - K8s NetworkPolicy with pod selector for kube-dns (original) - K8s NetworkPolicy with ipBlock for kube-dns ClusterIP - K8s NetworkPolicy with port-only DNS rule (no destination) - K8s NetworkPolicy with ingress-only (no egress policyType) - CiliumNetworkPolicy with <code>dns: [{matchPattern: \"*\"}]</code> L7 rules - CiliumNetworkPolicy without L7 rules</p> <p>Current fix: NetworkPolicy removed entirely for auntalma pods. All other egress restrictions are removed.</p> <p>UPDATE: DNS fails intermittently even WITHOUT NetworkPolicy (~60% failure rate on <code>gethostbyname</code>/<code>getaddrinfo</code>). Cilium 1.18.6 has <code>dnsproxy-enable-transparent-mode: true</code> by default, meaning the DNS proxy intercepts ALL DNS traffic regardless of whether policies exist.</p> <p>Permanent fix \u2014 Cilium config change (recommended):</p> <p>Add to <code>kubernetes/apps/base/kube-system/cilium/app/values.yaml</code>: <pre><code>dnsProxy:\n  dnsRejectResponseCode: nameError\n</code></pre></p> <p>This changes Cilium from returning <code>REFUSED</code> (musl aborts on this) to <code>NXDOMAIN</code> (musl handles correctly and continues searching). Cluster-wide fix, no image changes needed.</p> <p>Source: Cilium DNS + glibc resolver, cilium/cilium#33144</p> <p>Other options (if Cilium fix insufficient): - Option A: Switch from Alpine to Debian-based PHP image (glibc handles DNS correctly) - Option B: Add <code>dnsConfig</code> to pod spec \u2014 note: <code>single-request-reopen</code> is glibc-only, musl only supports <code>ndots</code>, <code>timeout</code>, <code>attempts</code> - Option C: Use <code>hostAliases</code> in pod spec to bypass DNS entirely (fragile \u2014 ClusterIPs can change)</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#28-k8s-networkpolicy-cilium-broken-pod-networking-on-some-nodes","title":"28. K8s NetworkPolicy + Cilium = Broken Pod Networking on Some Nodes","text":"<p>Symptom: Pods on worker2/worker3 have zero network connectivity \u2014 can't ping kube-dns ClusterIP, can't reach any pod IP, can't resolve DNS. But worker1 pods with the same spec work fine. Cilium endpoint shows <code>ready</code>. Plain Alpine pods without NetworkPolicy work fine on the same nodes.</p> <p>Root Cause: Kubernetes NetworkPolicy (even ingress-only with NO egress rules) triggers Cilium to attach eBPF programs for policy enforcement. On some nodes, these programs break the pod's entire datapath \u2014 not just DNS, but ALL traffic. The issue is node-specific and non-deterministic.</p> <p>Key findings: - Removing the NetworkPolicy entirely restores networking immediately - The issue is NOT related to egress rules \u2014 even <code>policyTypes: [Ingress]</code> with no egress section triggers it - Restarting the cilium agent on the affected node does NOT fix it - Deleting and recreating the pod does NOT fix it (new pod on same node also broken) - Plain pods (no NetworkPolicy matching them) work fine on the same nodes - This is likely a Cilium bug with <code>datapathMode: netkit</code> (eBPF-based datapath)</p> <p>Current fix: NetworkPolicy changed to ingress-only (egress enforcement removed). But on some nodes, even this triggers the issue.</p> <p>Workaround for affected deployments: 1. Scale to 1 replica pinned to a working node 2. Or remove NetworkPolicy entirely until Cilium fix is available</p> <p>Related: Issue #27 (DNS-specific symptoms were actually this broader networking issue)</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#29-kubectl-set-env-triggers-rollout-loses-all-in-container-patches","title":"29. <code>kubectl set env</code> Triggers Rollout \u2014 Loses All In-Container Patches","text":"<p>Symptom: After using <code>kubectl set env</code> to add IP-based env vars (bypassing DNS), all pages go 500 with \"Unable to retrieve deployment version of static files\" or \"The configuration file has changed.\"</p> <p>Root Cause: <code>kubectl set env</code> modifies the deployment spec, triggering a pod rollout. New pods start from the original Docker image, losing: - di.xml MariaDB 12 patch - Compiled DI code (<code>generated/metadata/</code> and <code>generated/code/</code>) - Static content deployed to emptyDir - Any <code>app:config:import</code> state</p> <p>Lesson: NEVER use <code>kubectl set env</code> during migration. Instead, pass env var overrides via <code>kubectl exec ... bash -c \"export VAR=val &amp;&amp; php bin/magento ...\"</code>. This runs commands with overridden env vars WITHOUT modifying the deployment spec or triggering a rollout.</p> <p>If you already did it: You need to re-run the full Magento post-migration sequence (di.xml patch, clear generated, setup:upgrade, di:compile, static-content:deploy, indexer:reindex, cache:flush) on the new pods.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#30-appconfigimport-required-after-config-changes","title":"30. <code>app:config:import</code> Required After Config Changes","text":"<p>Symptom: All pages return 500 with: <pre><code>The configuration file has changed. Run the \"app:config:import\" or the \"setup:upgrade\" command to synchronize the configuration.\n</code></pre></p> <p>Root Cause: Magento's <code>ConfigChangeDetector</code> compares a hash of <code>config.php</code> + <code>env.php</code> against a stored hash in the DB. If they differ (e.g., after env var changes, ConfigMap updates, or import operations), all requests are blocked.</p> <p>Fix: <pre><code>kubectl exec $POD -n business-system -c php -- php /var/www/html/bin/magento app:config:import\n</code></pre></p> <p>Important: This command itself can trigger the static content version error (Issue #16) because it updates the config version hash. After running it, you may need to redeploy static content.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#31-envphp-uses-non-obvious-env-var-names","title":"31. env.php Uses Non-Obvious Env Var Names","text":"<p>Symptom: Setting <code>MAGENTO_ELASTICSEARCH_HOST</code> or <code>MAGENTO_REDIS_HOST</code> has no effect \u2014 Magento still uses default <code>localhost</code>.</p> <p>Root Cause: The env.php ConfigMap reads specific env var names that don't match intuitive guesses:</p> Service Correct Env Var Wrong Guess Elasticsearch <code>MAGENTO_ES_HOST</code> <code>MAGENTO_ELASTICSEARCH_HOST</code> Session Redis <code>MAGENTO_SESSION_REDIS_HOST</code> <code>MAGENTO_REDIS_HOST</code> Cache Redis <code>MAGENTO_CACHE_REDIS_HOST</code> <code>MAGENTO_REDIS_HOST</code> Page Cache Redis <code>MAGENTO_PAGE_CACHE_REDIS_HOST</code> <code>MAGENTO_REDIS_HOST</code> <p>Lesson: Always check <code>configmap-env-php.yaml</code> for the exact <code>getenv()</code> calls before setting env vars. Each Redis instance (session, cache, page cache) has its own host/port/db env vars.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#32-homepage-404-multiple-overlapping-causes","title":"32. Homepage 404 \u2014 Multiple Overlapping Causes","text":"<p>Symptom: Homepage <code>/</code> returns static 404 (659 bytes) while all other pages work (customer login 200/83KB, search 200, contact 200, cart 200).</p> <p>Root Cause (updated Feb 11): This was misdiagnosed as a pure ES PHP client issue. Investigation revealed three overlapping causes, each producing the same 404 symptom:</p> <ol> <li> <p><code>ConfigChangeDetector</code> blocking all requests (primary cause): After Helm release history was nuked and Flux did a fresh install, the new pods had a DB schema mismatch (<code>setup:db:status</code> \u2192 \"Declarative Schema is not up to date\"). Magento's <code>ConfigChangeDetector</code> intercepts ALL requests and throws <code>LocalizedException</code> before any routing occurs. The error handler renders the static 404 page. The exception log deduplicates identical exceptions, so subsequent hits don't log new entries \u2014 making it look like \"no error is thrown.\"</p> </li> <li> <p>ES PHP client intermittent ClusterIP timeouts (secondary cause): Even with DNS working, PHP's <code>curl_init()</code> to the ES ClusterIP intermittently times out (10s). The ES <code>elasticsearch/elasticsearch</code> library built on top of curl also fails. However, the same ClusterIP works via <code>curl</code> CLI from the same pod \u2014 the issue is specific to PHP's curl extension/socket handling on Alpine. Setting the ClusterIP directly in <code>core_config_data</code> AND env var overrides together was needed to bypass this (see Issue #38).</p> </li> <li> <p><code>setup:upgrade</code> wipes static content (tertiary cause): After running <code>setup:upgrade</code> to fix cause #1, it clears <code>pub/static/</code> (including <code>deployed_version.txt</code> copied by the init container), breaking ALL pages with \"Unable to retrieve deployment version of static files\" (see Issue #37).</p> </li> </ol> <p>Diagnosis flow: <pre><code>Initial symptom: homepage 404 (659 bytes)\n\u251c\u2500 CMS config? \u2192 No: /home returns 200, cms_home_page=home, page_id 2 exists\n\u251c\u2500 ES PHP client? \u2192 Yes, partially: exception log shows \"No alive nodes\"\n\u2502   \u2514\u2500 But: DNS works (gethostbyname resolves), curl works \u2192 PHP curl timeout issue\n\u251c\u2500 ConfigChangeDetector? \u2192 YES: setup:db:status shows schema out of date\n\u2502   \u2514\u2500 Fix: setup:upgrade + di:compile + app:config:import\n\u2514\u2500 After fix: setup:upgrade wipes pub/static \u2192 need SCD or pod restart\n</code></pre></p> <p>Key lesson: When Magento returns a static 404 page (from <code>pub/errors/</code>), check <code>var/report/</code> for error details \u2014 the exception log may be deduplicating. The <code>ConfigChangeDetector</code> error is silent after the first log entry and blocks ALL routes before any CMS/catalog logic runs.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#33-helm-release-history-poisoning","title":"33. Helm Release History Poisoning","text":"<p>Symptom: HelmRelease stuck in upgrade/rollback loop. <code>helm history</code> shows 400+ failed revisions, all rolling back to the same old revision. New upgrades fail with <code>context deadline exceeded</code> before pods even start.</p> <p>Root Cause: Each failed upgrade + automatic rollback creates 2 Helm release secrets. After hundreds of cycles, Helm spends most of its timeout just loading release history. The release never cleans up because every attempt fails.</p> <p>Fix: Delete all Helm release secrets to force a fresh install: <pre><code># Suspend Flux first\nflux suspend helmrelease &lt;site&gt; -n business-system\n\n# Delete ALL helm release history (forces fresh install on next reconcile)\nkubectl delete secrets -n business-system -l owner=helm,name=&lt;site&gt;\n\n# Resume Flux \u2014 will do a fresh install (revision 1)\nflux resume helmrelease &lt;site&gt; -n business-system\n</code></pre></p> <p>Warning: This is a nuclear option \u2014 Helm loses all rollback history. Only use when the release is already broken beyond repair. Make sure your HelmRelease values are correct before resuming.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#34-helmrelease-upgrade-timeout-must-be-10m-for-magento","title":"34. HelmRelease Upgrade Timeout Must Be 10m+ for Magento","text":"<p>Symptom: HelmRelease upgrade completes successfully (pods reach 2/2 Ready) but Helm still rolls back. <code>helm history</code> shows the upgrade as <code>failed</code> despite pods being healthy.</p> <p>Root Cause: Magento pods have long <code>initialDelaySeconds</code> on probes (120s for liveness, 30s for readiness). The default Helm upgrade timeout is 5 minutes. With init containers (static-copy ~30s) + readiness delay (30s) + actual startup time, pods can take 3-4 minutes to become Ready. On slower nodes or with image pulls, this exceeds 5 minutes. Helm marks the upgrade as failed and rolls back.</p> <p>Fix: Add explicit upgrade timeout to the HelmRelease: <pre><code>spec:\n  upgrade:\n    timeout: 10m\n</code></pre></p> <p>Note: This timeout covers the entire upgrade operation including waiting for all pods to become Ready, not just the Helm template rendering. For Magento with 2+ replicas, 10 minutes is a safe buffer.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#35-setupupgrade-fails-with-es-but-appconfigimport-works","title":"35. <code>setup:upgrade</code> Fails with ES but <code>app:config:import</code> Works","text":"<p>Symptom: <code>setup:upgrade</code> fails with: <pre><code>No alive nodes found in your cluster\n</code></pre> Even though <code>curl http://auntalma-elasticsearch-es-http:9200</code> returns a healthy response from the same pod.</p> <p>Root Cause: Magento's <code>setup:upgrade</code> validates Elasticsearch connectivity using the PHP <code>elasticsearch/elasticsearch</code> client library, which performs its own DNS resolution and connection pooling. The PHP ES client may fail where raw HTTP succeeds due to: - musl DNS flakiness in the PHP runtime (different socket behavior than curl) - Connection timeout defaults in the PHP ES client - The client performing a \"sniff\" operation that resolves to unreachable node IPs</p> <p>Workaround: Use <code>app:config:import</code> instead of <code>setup:upgrade</code> when you only need to sync the config version hash (i.e., after env.php or config.php changes). <code>app:config:import</code> doesn't validate ES connectivity: <pre><code>kubectl exec $POD -n business-system -c php -- php /var/www/html/bin/magento app:config:import\nkubectl exec $POD -n business-system -c php -- php /var/www/html/bin/magento cache:flush\n</code></pre></p> <p>When to use which: - <code>setup:upgrade</code>: Required after DB schema changes, module install/upgrade, or major version bumps - <code>app:config:import</code>: Sufficient when only config files (env.php, config.php) have changed \u2014 syncs the config version hash in the DB</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#36-hpa-minreplicas-overrides-manual-kubectl-scale","title":"36. HPA MinReplicas Overrides Manual <code>kubectl scale</code>","text":"<p>Symptom: You scale the deployment to 1 replica with <code>kubectl scale --replicas=1</code>, but within seconds it scales back up to 2.</p> <p>Root Cause: The HorizontalPodAutoscaler (HPA) has <code>minReplicas: 2</code> configured. HPA continuously reconciles and overrides any manual scaling that drops below its minimum.</p> <p>Fix: Patch the HPA's minReplicas to allow single-replica operation: <pre><code>kubectl patch hpa &lt;site&gt; -n business-system --type='json' \\\n  -p='[{\"op\": \"replace\", \"path\": \"/spec/minReplicas\", \"value\": 1}]'\n</code></pre></p> <p>Important: Remember to restore minReplicas when multi-node operation is fixed. This is a temporary workaround for the Cilium + NetworkPolicy issue (Issue #28) that prevents pods on worker2/worker3 from having network connectivity.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#37-setupupgrade-wipes-pubstatic-init-container-content-lost","title":"37. <code>setup:upgrade</code> Wipes <code>pub/static</code> \u2014 Init Container Content Lost","text":"<p>Symptom: After running <code>setup:upgrade</code>, ALL pages break with \"Unable to retrieve deployment version of static files\" or render without CSS/JS.</p> <p>Root Cause: <code>setup:upgrade</code> runs file system cleanup that deletes: <pre><code>/var/www/html/pub/static/adminhtml\n/var/www/html/pub/static/deployed_version.txt\n/var/www/html/pub/static/frontend\n/var/www/html/var/view_preprocessed/pub\n</code></pre></p> <p>The init container (<code>static-copy</code>) copies baked-in static files at pod startup, but <code>setup:upgrade</code> wipes them. Since <code>pub/static</code> is an emptyDir, there's no image content to fall back to.</p> <p>Fix: After running <code>setup:upgrade</code>, you MUST either: - Option A: Restart the pod (triggers init container to re-copy static files) - Option B: Run <code>setup:static-content:deploy</code> manually: <pre><code>kubectl exec $POD -n business-system -c php -- bash -c \"\ncd /var/www/html &amp;&amp; php bin/magento setup:static-content:deploy en_US en_AU -f\n\"\n</code></pre></p> <p>Important: This means the full post-migration sequence is: <code>setup:upgrade</code> \u2192 <code>di:compile</code> \u2192 pod restart (or SCD) \u2192 <code>app:config:import</code> \u2192 <code>cache:flush</code>. The pod restart is needed between <code>di:compile</code> and <code>app:config:import</code> to restore static content from the init container.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#38-es-php-client-clusterip-timeouts-core_config_data-overrides-envphp","title":"38. ES PHP Client ClusterIP Timeouts \u2014 <code>core_config_data</code> Overrides env.php","text":"<p>Symptom: <code>setup:upgrade</code> fails with \"No alive nodes found in your cluster\" even when: - DNS resolves correctly (<code>gethostbyname</code> returns correct IP) - <code>curl</code> to ES service works from the same pod - env var <code>MAGENTO_ES_HOST</code> is set to the ClusterIP</p> <p>Root Cause: Two issues compound:</p> <ol> <li> <p><code>core_config_data</code> overrides <code>system.default</code> in env.php for already-configured stores. Setting <code>MAGENTO_ES_HOST</code> env var only populates <code>system.default.catalog.search.elasticsearch7_server_hostname</code> in env.php. But if the DB already has <code>catalog/search/elasticsearch7_server_hostname</code> in <code>core_config_data</code>, the DB value takes precedence at runtime.</p> </li> <li> <p>PHP curl to ClusterIP intermittently times out on Alpine: Raw <code>curl_init()</code> to the ES ClusterIP from PHP times out (10s) even when CLI <code>curl</code> works. The ES PHP client library (<code>elasticsearch/elasticsearch</code>) uses PHP curl internally. The issue is intermittent and may be related to musl's socket handling or Cilium's eBPF datapath.</p> </li> </ol> <p>Fix: Update BOTH the env var AND the <code>core_config_data</code>: <pre><code># 1. Set ClusterIP in core_config_data\nES_IP=$(kubectl get svc &lt;site&gt;-elasticsearch-es-http -n business-system -o jsonpath='{.spec.clusterIP}')\nkubectl exec &lt;site&gt;-mariadb-0 -n business-system -c mariadb -- bash -c \\\n  'mariadb -u root -p\"$MARIADB_ROOT_PASSWORD\" magento -e \"\n    UPDATE core_config_data SET value = '\"'\"''\"$ES_IP\"''\"'\"' WHERE path = '\"'\"'catalog/search/elasticsearch7_server_hostname'\"'\"';\n  \"'\n\n# 2. Flush Redis after DB config change\nkubectl exec $POD -n business-system -c php -- php -r \"\n\\$r = new Redis(); \\$r-&gt;connect('$REDIS_IP', 6379);\n\\$r-&gt;select(6); \\$r-&gt;flushDB(); \\$r-&gt;select(1); \\$r-&gt;flushDB();\"\n\n# 3. Pass env var override in setup:upgrade command\nexport MAGENTO_ES_HOST=$ES_IP\n</code></pre></p> <p>Note: After resolving ES connectivity, revert <code>core_config_data</code> back to the DNS name for ongoing runtime use \u2014 the ClusterIP can change if the service is recreated.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#39-homepage-404-nginx-try_files-uri-with-split-containers","title":"39. Homepage 404 \u2014 nginx <code>try_files $uri/</code> With Split Containers","text":"<p>Symptom: Homepage <code>/</code> returns a static 404 (659 bytes from <code>pub/errors/default/</code>). ALL other Magento routes work (<code>/customer/account/login/</code> 200, <code>/catalogsearch/result/?q=test</code> 200, <code>/contact/</code> 200, <code>/home</code> 200). No Magento error report is generated.</p> <p>Root Cause: nginx error log reveals: <pre><code>directory index of \"/var/www/html/pub/\" is forbidden\n</code></pre></p> <p>In this bjw-s app-template architecture, nginx and PHP-FPM are separate containers. The nginx container is a plain <code>nginx:alpine</code> image that does NOT contain the Magento codebase. Only <code>pub/static</code> and <code>pub/media</code> are shared via emptyDir/PVC mounts. Crucially, <code>pub/index.php</code> does NOT exist in the nginx container's filesystem.</p> <p>The standard Magento nginx config has: <pre><code>location / {\n    try_files $uri $uri/ /index.php$is_args$args;\n}\n</code></pre></p> <p>For <code>GET /</code>, nginx evaluates <code>try_files</code> left to right: 1. <code>$uri</code> \u2192 <code>/</code> \u2192 checks <code>/var/www/html/pub/</code> as a file \u2192 it's a directory, skip 2. <code>$uri/</code> \u2192 <code>/</code> \u2192 checks <code>/var/www/html/pub/</code> as a directory \u2192 exists \u2192 nginx tries <code>index index.php</code> directive \u2192 <code>pub/index.php</code> doesn't exist on nginx filesystem \u2192 403 forbidden \u2192 <code>error_page 404 403 = /errors/404.php</code> \u2192 static 404 3. <code>/index.php$is_args$args</code> \u2192 never reached</p> <p>Other URLs like <code>/customer/account/login/</code> work because that directory doesn't exist on the nginx filesystem, so step 2 fails and step 3 (the PHP fallback) is reached.</p> <p>Fix: Remove <code>$uri/</code> from <code>try_files</code> in the nginx ConfigMap: <pre><code>location / {\n    try_files $uri /index.php$is_args$args;\n}\n</code></pre></p> <p>This is safe because: - Static files (<code>pub/static/</code>, <code>pub/media/</code>) are served by their own location blocks - All other requests should go to PHP-FPM via <code>index.php</code> - Directory listing is never needed in a Magento deployment</p> <p>Key lesson: Standard Magento nginx configs assume nginx and PHP share the same filesystem. In Kubernetes split-container deployments (separate nginx + PHP-FPM containers), <code>try_files $uri/</code> breaks for the root URL because <code>pub/</code> exists as a directory but <code>pub/index.php</code> doesn't. Any Magento nginx config for split containers must remove <code>$uri/</code> from <code>try_files</code>.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#40-appconfigimport-must-run-with-pods-native-env-vars","title":"40. <code>app:config:import</code> Must Run With Pod's Native Env Vars","text":"<p>Symptom: After running <code>app:config:import</code> with IP-overridden env vars (<code>export MAGENTO_DB_HOST=10.x.x.x &amp;&amp; php bin/magento app:config:import</code>), it reports \"Nothing to import.\" But the web process (PHP-FPM) still shows the <code>ConfigChangeDetector</code> error: \"The configuration file has changed.\"</p> <p>Root Cause: Magento's <code>ConfigChangeDetector</code> computes a hash of the evaluated configuration \u2014 not the raw file content, but the resolved array including all <code>getenv()</code> values. When you run <code>app:config:import</code> with overridden env vars, the stored hash reflects those IPs. But PHP-FPM uses the pod's actual env vars (DNS hostnames), producing a different hash \u2192 mismatch \u2192 all requests blocked.</p> <p>Fix: Run <code>app:config:import</code> via plain <code>kubectl exec</code> WITHOUT any <code>bash -c \"export ... &amp;&amp;\"</code> wrappers: <pre><code>kubectl exec $POD -n business-system -c php -- php /var/www/html/bin/magento app:config:import\nkubectl exec $POD -n business-system -c php -- php /var/www/html/bin/magento cache:flush\n</code></pre></p> <p>This uses the pod's native env vars, producing a hash that matches what PHP-FPM computes at runtime.</p> <p>Important: Use IP overrides only for <code>setup:upgrade</code> and <code>setup:di:compile</code> (which need reliable ES/DB connections). Always run <code>app:config:import</code> and <code>cache:flush</code> with native env vars as the FINAL step.</p> <p>Updated workflow order: 1. <code>setup:upgrade</code> (with IP overrides \u2014 ES connectivity required) 2. <code>setup:di:compile</code> (with IP overrides) 3. Pod restart (restores <code>pub/static</code> via init container) 4. <code>app:config:import</code> (native env vars \u2014 no overrides) 5. <code>cache:flush</code> (native env vars) 6. <code>indexer:reindex</code> (with IP overrides if needed for ES)</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#41-flux-fresh-install-recreates-deleted-networkpolicy","title":"41. Flux Fresh Install Recreates Deleted NetworkPolicy","text":"<p>Symptom: After deleting the NetworkPolicy to work around Issue #28, Flux does a fresh install (after Helm history reset) and the NetworkPolicy comes back, breaking networking on worker2/worker3 again.</p> <p>Root Cause: The NetworkPolicy is part of the HelmRelease template. Any Flux reconciliation (install, upgrade) recreates it. <code>kubectl delete</code> is only a temporary fix that lasts until the next reconcile.</p> <p>Workaround: After each Flux reconcile, immediately delete the NetworkPolicy: <pre><code>kubectl delete networkpolicy auntalma-app -n business-system\n</code></pre></p> <p>Permanent fix needed: Either disable the NetworkPolicy in HelmRelease values, or resolve the Cilium + K8s NetworkPolicy compatibility issue (Issue #28).</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#42-css-not-loading-docker-image-only-has-mincss-store-has-minification-disabled","title":"42. CSS Not Loading \u2014 Docker Image Only Has <code>.min.css</code>, Store Has Minification Disabled","text":"<p>Symptom: All pages load HTML but no CSS/JS. Browser console shows <code>Refused to apply style from...</code> errors. Static asset URLs like <code>/static/version.../frontend/Rival/auntalma/en_AU/css/styles-m.css</code> return 404.</p> <p>Root Cause: Three compounding issues:</p> <ol> <li> <p>Docker image deploys <code>en_US</code> but store uses <code>en_AU</code> (Issue #17): The <code>pub/static</code> emptyDir only has files under <code>en_US/</code>. Magento generates HTML with <code>en_AU/</code> URLs. All <code>en_AU</code> static files 404.</p> </li> <li> <p>Docker image only has <code>.min.css</code> files: The SCD during Docker build runs without a database connection. Without DB access, LESS\u2192CSS compilation is limited. Only <code>.min.css</code> files are generated (e.g., <code>styles-m.min.css</code>), not the unminified versions (<code>styles-m.css</code>).</p> </li> <li> <p>CSS/JS minification is disabled in <code>core_config_data</code>: With <code>dev/css/minify_files=0</code>, Magento generates HTML <code>&lt;link&gt;</code> tags referencing <code>styles-m.css</code> (non-minified). But only <code>styles-m.min.css</code> exists. The VPS has the same setting (minification disabled) but it works because the VPS SCD generates both minified and non-minified files (it has DB access during deploy).</p> </li> </ol> <p>Diagnosis: <pre><code># Check what locales have static files\nkubectl exec $POD -n business-system -c php -- ls /var/www/html/pub/static/frontend/Rival/auntalma/\n# Result: en_AU en_US \u2014 but en_AU only has 2 requirejs files, no CSS\n\n# Check CSS files\nkubectl exec $POD -n business-system -c php -- ls /var/www/html/pub/static/frontend/Rival/auntalma/en_US/css/styles*\n# Result: styles-l.min.css styles-m.min.css \u2014 no non-minified versions\n\n# Check minification config\nkubectl exec &lt;site&gt;-mariadb-0 -n business-system -c mariadb -- bash -c \\\n  'mariadb -u root -p\"$MARIADB_ROOT_PASSWORD\" magento -e \"\n    SELECT path, value FROM core_config_data WHERE path LIKE '\"'\"'dev/%/minify%'\"'\"' OR path LIKE '\"'\"'dev/%/merge%'\"'\"';\"'\n# Result: all 0 (disabled)\n</code></pre></p> <p>Fix (temporary \u2014 lost on pod restart): <pre><code># 1. Copy en_US files to en_AU\nkubectl exec $POD -n business-system -c php -- bash -c \\\n  \"cp -a /var/www/html/pub/static/frontend/Rival/auntalma/en_US/* \\\n   /var/www/html/pub/static/frontend/Rival/auntalma/en_AU/\"\n\n# 2. Enable CSS/JS minification so Magento references .min.css files\nkubectl exec &lt;site&gt;-mariadb-0 -n business-system -c mariadb -- bash -c \\\n  'mariadb -u root -p\"$MARIADB_ROOT_PASSWORD\" magento -e \"\n    UPDATE core_config_data SET value = 1 WHERE path = '\"'\"'dev/css/minify_files'\"'\"';\n    UPDATE core_config_data SET value = 1 WHERE path = '\"'\"'dev/js/minify_files'\"'\"';\n    UPDATE core_config_data SET value = 1 WHERE path = '\"'\"'dev/css/merge_css_files'\"'\"';\n    UPDATE core_config_data SET value = 1 WHERE path = '\"'\"'dev/js/merge_files'\"'\"';\n  \"'\n\n# 3. Flush Magento cache\nkubectl exec $POD -n business-system -c php -- php /var/www/html/bin/magento cache:flush\n\n# 4. Bump deployed_version.txt to bust Cloudflare cached 404s\nkubectl exec $POD -n business-system -c php -- bash -c \\\n  \"echo \\$(date +%s) &gt; /var/www/html/pub/static/deployed_version.txt\"\n\n# 5. Flush cache again for new version\nkubectl exec $POD -n business-system -c php -- php /var/www/html/bin/magento cache:flush\n</code></pre></p> <p>Fix (permanent \u2014 Docker image change): 1. Update <code>Dockerfile.prod</code> frontend SCD: change <code>en_US</code> to <code>en_AU</code> (line 77). No composer changes needed \u2014 <code>en_AU</code> is a valid ICU locale and doesn't require a language pack for SCD (see Issue #17). 2. Optionally: decide whether minification should be enabled in <code>core_config_data</code> for K8s (it's disabled on VPS but VPS has full SCD output)</p> <p>Key lesson: <code>Dockerfile.prod</code> must match <code>deploy.php</code> locale configuration. The VPS deployer (<code>deploy.php</code> line 14) deploys <code>en_AU</code>; the Dockerfile must do the same. The misleading comment \"en_AU falls back to en_US at runtime\" is wrong \u2014 Magento has no static file locale fallback.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#43-cloudflare-caches-404-responses-for-static-assets","title":"43. Cloudflare Caches 404 Responses for Static Assets","text":"<p>Symptom: After fixing static files on disk, HTTP requests still return 404. But requests with a cache-busting query parameter (<code>?nocache=...</code>) return 200 with correct content. Response header <code>cf-cache-status: MISS</code> confirms Cloudflare bypass.</p> <p>Root Cause: Cloudflare caches 404 responses for static asset URLs. Since Magento uses versioned URLs (<code>/static/version1769728396/...</code>), the same URL is requested repeatedly and the cached 404 persists until Cloudflare TTL expires.</p> <p>Fix: Bump <code>deployed_version.txt</code> to force new versioned URLs that bypass the cached 404s: <pre><code>kubectl exec $POD -n business-system -c php -- bash -c \\\n  \"echo \\$(date +%s) &gt; /var/www/html/pub/static/deployed_version.txt\"\nkubectl exec $POD -n business-system -c php -- php /var/www/html/bin/magento cache:flush\n</code></pre></p> <p>This is safe because the version number in the URL is only used for cache busting \u2014 nginx strips it before serving the file.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#44-base-url-scope-override-scope_id1-overrides-scope_id0","title":"44. Base URL Scope Override \u2014 <code>scope_id=1</code> Overrides <code>scope_id=0</code>","text":"<p>Symptom: After updating base URLs in <code>core_config_data</code> (scope_id=0) to the test domain, all pages still redirect 302 to the production domain (<code>https://www.auntalma.com.au/</code>). Internal curl from the pod also redirects.</p> <p>Root Cause: Magento's <code>core_config_data</code> has scoped values. When <code>MAGE_RUN_CODE=base</code> is set, Magento runs as website ID 1 (scope <code>websites</code>, scope_id=1). If scope_id=1 has its own <code>web/unsecure/base_url</code> and <code>web/secure/base_url</code> entries, they override the default scope (scope_id=0).</p> <p>The production database has base URLs at both scopes: <pre><code>-- scope_id=0 (default) \u2014 updated during migration \u2713\nweb/unsecure/base_url = https://auntalma.haydenagencies.com.au/\nweb/secure/base_url   = https://auntalma.haydenagencies.com.au/\n\n-- scope_id=1 (website \"base\") \u2014 NOT updated, still points to production \u2717\nweb/unsecure/base_url = https://www.auntalma.com.au/\nweb/secure/base_url   = https://www.auntalma.com.au/\n</code></pre></p> <p>Diagnosis: <pre><code>SELECT config_id, scope, scope_id, path, value\nFROM core_config_data\nWHERE path LIKE 'web/%/base_url'\nORDER BY path, scope_id;\n</code></pre></p> <p>Fix: Update base URLs for ALL scopes, not just the default: <pre><code>-- Update scope_id=0 (default)\nUPDATE core_config_data SET value = 'https://&lt;test-domain&gt;/'\n  WHERE path IN ('web/unsecure/base_url', 'web/secure/base_url')\n  AND scope_id = 0;\n\n-- Update scope_id=1 (website scope for MAGE_RUN_CODE)\nUPDATE core_config_data SET value = 'https://&lt;test-domain&gt;/'\n  WHERE path IN ('web/unsecure/base_url', 'web/secure/base_url')\n  AND scope_id = 1;\n</code></pre></p> <p>Key lesson: Always query ALL scopes for base URLs after import. The website-level scope (matching <code>MAGE_RUN_CODE</code>) takes precedence over the default scope. Multi-store setups may have additional scopes (scope_id=2, 3, etc.) that also need updating.</p> <p>Important: Flush Redis (DB 6 + DB 1) and run <code>cache:flush</code> after updating base URLs \u2014 Magento caches config values aggressively.</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#proven-working-workflow","title":"Proven Working Workflow","text":"<p>All-in-one command sequence for running <code>setup:upgrade</code> and completing a Magento migration. Replace <code>&lt;site&gt;</code> with the actual site name (e.g., <code>auntalma</code>, <code>hayden</code>).</p> <p>Note: After applying the Cilium DNS fix (Issue #27, commit <code>b642d7f5d</code>), short hostnames resolve reliably for most operations. However, <code>setup:upgrade</code> ES validation still intermittently fails with DNS hostnames \u2014 IP overrides for ES are required (see Issue #38). You must also update <code>core_config_data</code> to use the ES ClusterIP, since DB config overrides env.php.</p> <p>WARNING: <code>setup:upgrade</code> wipes <code>pub/static/</code> (Issue #37). After the full sequence, either restart the pod (init container re-copies static files) or run <code>setup:static-content:deploy</code> manually.</p> <p>WARNING: After pod restart, the init container only copies <code>en_US</code> static files (Dockerfile deploys <code>en_US</code> instead of <code>en_AU</code>). You MUST copy <code>en_US</code> to <code>en_AU</code> after every pod restart until <code>Dockerfile.prod</code> is fixed to deploy <code>en_AU</code> (Issue #17, #42). Also ensure CSS/JS minification is enabled in <code>core_config_data</code> (the Docker image only has <code>.min.css</code> files).</p> <p>CRITICAL: Pass env overrides via <code>export</code> inside <code>kubectl exec ... bash -c \"...\"</code>. NEVER use <code>kubectl set env</code> \u2014 it modifies the deployment spec and triggers a rollout, destroying all in-container patches (Issue #29).</p> <p>CRITICAL: <code>app:config:import</code> and <code>cache:flush</code> must run with the pod's native env vars (no overrides). The config hash must match what PHP-FPM computes at runtime. Use IP overrides only for <code>setup:upgrade</code>, <code>di:compile</code>, and <code>indexer:reindex</code>. (Issue #40)</p> <pre><code># 1. Suspend Flux to prevent pod replacement during migration\nflux suspend helmrelease &lt;site&gt; -n business-system\n\n# 2. Get pod name and resolve service IPs\nPOD=$(kubectl get pod -n business-system -l app.kubernetes.io/instance=&lt;site&gt; \\\n  --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}')\n\nREDIS_IP=$(kubectl get svc &lt;site&gt;-redis-master -n business-system -o jsonpath='{.spec.clusterIP}')\nDB_IP=$(kubectl get pod &lt;site&gt;-mariadb-0 -n business-system -o jsonpath='{.status.podIP}')  # pod IP \u2014 ClusterIP unreliable\nES_IP=$(kubectl get svc &lt;site&gt;-elasticsearch-es-http -n business-system -o jsonpath='{.spec.clusterIP}')\nAMQP_IP=$(kubectl get svc &lt;site&gt;-rabbitmq -n business-system -o jsonpath='{.spec.clusterIP}')\n\n# 3. Update ES hostname in core_config_data to ClusterIP (DB config overrides env.php \u2014 Issue #38)\nkubectl exec &lt;site&gt;-mariadb-0 -n business-system -c mysql -- bash -c \\\n  'mysql -u root -p\"$MARIADB_ROOT_PASSWORD\" magento -e \"\n    UPDATE core_config_data SET value = '\"'\"''\"$ES_IP\"''\"'\"' WHERE path = '\"'\"'catalog/search/elasticsearch7_server_hostname'\"'\"';\n  \"'\n\n# 4. Clear caches and run setup:upgrade\nkubectl exec $POD -n business-system -c php -- bash -c \"\n# Clear generated code + caches\nrm -rf /var/www/html/generated/code/* /var/www/html/generated/metadata/* /var/www/html/var/cache/* /var/www/html/var/di/*\n\n# Flush Redis using IP\nphp -r \\\"\\\\\\$r = new Redis(); \\\\\\$r-&gt;connect('$REDIS_IP', 6379); \\\\\\$r-&gt;select(6); \\\\\\$r-&gt;flushDB(); \\\\\\$r-&gt;select(1); \\\\\\$r-&gt;flushDB();\\\"\n\n# Override env vars with IPs and run setup:upgrade\nexport MAGENTO_DB_HOST=$DB_IP\nexport MAGENTO_SESSION_REDIS_HOST=$REDIS_IP\nexport MAGENTO_CACHE_REDIS_HOST=$REDIS_IP\nexport MAGENTO_PAGE_CACHE_REDIS_HOST=$REDIS_IP\nexport MAGENTO_ES_HOST=$ES_IP\nexport MAGENTO_AMQP_HOST=$AMQP_IP\nexport MAGE_RUN_CODE=base\ncd /var/www/html &amp;&amp; php bin/magento setup:upgrade\n\"\n\n# 5. Run di:compile (required after setup:upgrade)\nkubectl exec $POD -n business-system -c php -- bash -c \"\nexport MAGENTO_DB_HOST=$DB_IP\nexport MAGENTO_SESSION_REDIS_HOST=$REDIS_IP\nexport MAGENTO_CACHE_REDIS_HOST=$REDIS_IP\nexport MAGENTO_PAGE_CACHE_REDIS_HOST=$REDIS_IP\nexport MAGENTO_ES_HOST=$ES_IP\nexport MAGENTO_AMQP_HOST=$AMQP_IP\nexport MAGE_RUN_CODE=base\ncd /var/www/html &amp;&amp; php bin/magento setup:di:compile\n\"\n\n# 6. Restore static content (setup:upgrade wipes pub/static \u2014 Issue #37)\n# Option A: Restart pod to trigger init container (recommended)\nkubectl delete pod $POD -n business-system\n# Wait for new pod to be ready\nkubectl wait --for=condition=Ready pod -n business-system -l app.kubernetes.io/instance=&lt;site&gt; --timeout=5m\n# Re-resolve pod name after restart\nPOD=$(kubectl get pod -n business-system -l app.kubernetes.io/instance=&lt;site&gt; \\\n  --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}')\n\n# Option B: OR deploy static content manually (slower, but no pod restart)\n# kubectl exec $POD -n business-system -c php -- bash -c \"\n# cd /var/www/html &amp;&amp; php bin/magento setup:static-content:deploy en_US en_AU -f\n# \"\n\n# 6b. Copy en_US static files to en_AU (Docker image only has en_US \u2014 Issue #42)\nkubectl exec $POD -n business-system -c php -- bash -c \\\n  \"cp -a /var/www/html/pub/static/frontend/Rival/auntalma/en_US/* \\\n   /var/www/html/pub/static/frontend/Rival/auntalma/en_AU/\"\n\n# 6c. Ensure CSS/JS minification is enabled (Docker image only has .min.css \u2014 Issue #42)\nkubectl exec &lt;site&gt;-mariadb-0 -n business-system -c mysql -- bash -c \\\n  'mysql -u root -p\"$MARIADB_ROOT_PASSWORD\" magento -e \"\n    UPDATE core_config_data SET value = 1 WHERE path IN (\n      '\"'\"'dev/css/minify_files'\"'\"', '\"'\"'dev/js/minify_files'\"'\"',\n      '\"'\"'dev/css/merge_css_files'\"'\"', '\"'\"'dev/js/merge_files'\"'\"'\n    );\"'\n\n# 7. Sync config hash and flush caches\n# CRITICAL: Do NOT use env overrides here! The config hash must match what PHP-FPM\n# computes at runtime using the pod's native env vars. (Issue #40)\nkubectl exec $POD -n business-system -c php -- php /var/www/html/bin/magento app:config:import\nkubectl exec $POD -n business-system -c php -- php /var/www/html/bin/magento cache:flush\n\n# 8. Run indexer\nkubectl exec $POD -n business-system -c php -- bash -c \"\nexport MAGENTO_DB_HOST=$DB_IP\nexport MAGENTO_SESSION_REDIS_HOST=$REDIS_IP\nexport MAGENTO_CACHE_REDIS_HOST=$REDIS_IP\nexport MAGENTO_PAGE_CACHE_REDIS_HOST=$REDIS_IP\nexport MAGENTO_ES_HOST=$ES_IP\nexport MAGENTO_AMQP_HOST=$AMQP_IP\nexport MAGE_RUN_CODE=base\ncd /var/www/html &amp;&amp; php bin/magento indexer:reindex\n\"\n\n# 9. Revert ES core_config_data back to DNS name (ClusterIP may change on service recreation)\nkubectl exec &lt;site&gt;-mariadb-0 -n business-system -c mysql -- bash -c \\\n  'mysql -u root -p\"$MARIADB_ROOT_PASSWORD\" magento -e \"\n    UPDATE core_config_data SET value = '\"'\"'&lt;site&gt;-elasticsearch-es-http'\"'\"' WHERE path = '\"'\"'catalog/search/elasticsearch7_server_hostname'\"'\"';\n  \"'\nkubectl exec $POD -n business-system -c php -- bash -c \"\nphp -r \\\"\\\\\\$r = new Redis(); \\\\\\$r-&gt;connect('$REDIS_IP', 6379); \\\\\\$r-&gt;select(6); \\\\\\$r-&gt;flushDB(); \\\\\\$r-&gt;select(1); \\\\\\$r-&gt;flushDB();\\\"\n\"\n\n# 10. Resume Flux\nflux resume helmrelease &lt;site&gt; -n business-system\n</code></pre>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#complete-migration-checklist","title":"Complete Migration Checklist","text":"<p>For each Magento site (auntalma, hayden, toemass/dropdrape):</p>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#pre-requisites-fix-before-migration","title":"Pre-requisites (fix BEFORE migration)","text":"<ul> <li> Fix Cilium DNS proxy for musl compat \u2014 add <code>dnsProxy.dnsRejectResponseCode: nameError</code> to Cilium values (Issue #27)</li> <li> Add init container to HelmRelease for <code>pub/static</code> deployment (Issue #16)</li> <li> Set HelmRelease upgrade timeout to 10m (Issue #34)</li> <li> Pin MariaDB to 10.11 LTS in <code>mariadb-cluster.yaml</code> + Renovate <code>allowedVersions</code> rule (Issue #2)</li> <li> Change <code>Dockerfile.prod</code> frontend SCD from <code>en_US</code> to <code>en_AU</code> (Issue #17, #42)</li> <li> Add <code>install.date</code> to env.php ConfigMap (Issue #15)</li> <li> Use short service names in helmrelease env vars (Issue #7)</li> <li> Ensure <code>log_bin_trust_function_creators=1</code> in myCnf (Issue #10)</li> <li> Fix HTTPRoute <code>backendRefs</code> to match actual service name (Issue #24)</li> <li> Add external-dns annotations to HTTPRoute (Issue #25)</li> <li> Fix NetworkPolicy to allow traffic from <code>network-system</code> envoy pods (Issue #26)</li> <li> Verify env var names match <code>configmap-env-php.yaml</code> <code>getenv()</code> calls (Issue #31)</li> <li> Resolve Cilium + K8s NetworkPolicy multi-node networking issue (Issue #28) \u2014 or remove NetworkPolicy</li> <li> Fix nginx <code>try_files</code> for split-container architecture \u2014 remove <code>$uri/</code> (Issue #39)</li> </ul>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#pre-flight-checks","title":"Pre-flight checks","text":"<ul> <li> Verify Galera cluster healthy (<code>kubectl get mariadb -n business-system</code>)</li> <li> Verify ES, Redis, RabbitMQ running</li> <li> Verify pre-requisites above are all in place</li> </ul>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#database-import","title":"Database import","text":"<ul> <li> Dump production DB from VPS</li> <li> Strip <code>LOCK TABLES</code> / <code>UNLOCK TABLES</code> from dump</li> <li> <code>kubectl cp</code> dump into MariaDB pod, import locally</li> <li> Query <code>store_website</code> and <code>store</code> tables for correct codes</li> <li> Update <code>MAGE_RUN_CODE</code> in helmrelease to match DB</li> <li> Update base URLs in <code>core_config_data</code> for test domain \u2014 ALL scopes (scope_id=0 AND website-specific scope_id, Issue #44)</li> <li> Update ES config in <code>core_config_data</code> to local cluster ES</li> <li> Update crypt key in SOPS secret to match production</li> <li> Flush Redis after any <code>core_config_data</code> changes</li> </ul>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#magento-setup-commands-suspend-helmrelease-first","title":"Magento setup commands (suspend HelmRelease first!)","text":"<ul> <li> <code>flux suspend helmrelease &lt;site&gt; -n business-system</code></li> <li> Delete NetworkPolicy if present (Issue #41 \u2014 Flux recreates it on install)</li> <li> Update ES <code>core_config_data</code> to ClusterIP (Issue #38 \u2014 DB config overrides env.php)</li> <li> Clear generated code: <code>rm -rf generated/code/* generated/metadata/* var/cache/* var/di/*</code></li> <li> Flush Redis (DB 6 + DB 1)</li> <li> Run <code>setup:upgrade</code> (with IP env overrides \u2014 especially <code>MAGENTO_ES_HOST</code>)</li> <li> Run <code>setup:di:compile</code> (with IP env overrides)</li> <li> Restore <code>pub/static</code> \u2014 restart pod (init container re-copies) OR run SCD (Issue #37)</li> <li> Run <code>app:config:import</code> (native env vars \u2014 NO overrides \u2014 Issue #40)</li> <li> Run <code>cache:flush</code> (native env vars \u2014 NO overrides)</li> <li> Run <code>indexer:reindex</code> (with IP env overrides if needed for ES)</li> <li> Revert ES <code>core_config_data</code> back to DNS name + flush Redis</li> <li> <code>flux resume helmrelease &lt;site&gt; -n business-system</code></li> </ul>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#media","title":"Media","text":"<ul> <li> Stream media files from VPS: <code>ssh ... tar | kubectl exec ... tar</code></li> <li> Fix ownership: <code>chown -R 1000:1000 /var/www/html/pub/media/</code></li> </ul>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#verify","title":"Verify","text":"<ul> <li> <code>curl -I https://&lt;test-domain&gt;/</code> returns 200</li> <li> Admin panel accessible at <code>/admin_hayden/</code></li> <li> Product pages load with CSS/JS (static content working)</li> <li> Cron jobs not erroring</li> </ul>"},{"location":"MAGENTO-PRODUCTION-MIGRATION/#key-commands-reference","title":"Key Commands Reference","text":"<pre><code># Remote access (when not on local network)\n# Requires cloudflared tunnel running: cloudflared access tcp --hostname api.haydenagencies.com.au --url 127.0.0.1:1234 &amp;\n# Prefix all kubectl/flux/helm commands with:\nHTTPS_PROXY=socks5://127.0.0.1:1234 kubectl ...\n\n# Get root password\nROOT_PW=$(kubectl get secret &lt;site&gt;-mariadb-superuser -n business-system -o jsonpath='{.data.password}' | base64 -d)\n\n# MariaDB CLI (10.11 uses 'mysql')\nkubectl exec &lt;site&gt;-mariadb-0 -n business-system -c mysql -- mysql -u root -p\"${ROOT_PW}\" magento -e \"...\"\n\n# Find running app pod\nPOD=$(kubectl get pod -n business-system -l app.kubernetes.io/instance=&lt;site&gt; --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}')\n\n# Get all service IPs (use when DNS is flaky \u2014 Issue #23)\nkubectl get svc -n business-system &lt;site&gt;-redis-master &lt;site&gt;-mariadb-primary \\\n  &lt;site&gt;-elasticsearch-es-http &lt;site&gt;-rabbitmq \\\n  -o jsonpath='{range .items[*]}{.metadata.name}={.spec.clusterIP}{\"\\n\"}{end}'\n\n# Flush Redis\nkubectl exec $POD -n business-system -c php -- php -r \"\\$r = new Redis(); \\$r-&gt;connect('&lt;site&gt;-redis-master', 6379); \\$r-&gt;select(6); \\$r-&gt;flushDB(); \\$r-&gt;select(1); \\$r-&gt;flushDB();\"\n\n# Emergency env var patch (bypasses Flux/Helm \u2014 Issue #21)\nkubectl set env deployment/&lt;site&gt; -n business-system -c php MAGE_RUN_CODE=base MAGENTO_DB_HOST=&lt;site&gt;-mariadb-primary ...\n\n# Force Flux reconcile\nkubectl annotate kustomization cluster -n flux-system reconcile.fluxcd.io/requestedAt=\"$(date +%s)\" --overwrite\n</code></pre>"},{"location":"ODOO-DUPLICATE-SKU-REMEDIATION/","title":"Odoo Duplicate SKU Remediation Tasks","text":"<p>Created: 2026-02-05 Issue: Duplicate products with same SKU causing stock mismatches and failed \"Validate Picking\" jobs</p>"},{"location":"ODOO-DUPLICATE-SKU-REMEDIATION/#problem-summary","title":"Problem Summary","text":"<p>The Magento-Odoo integration created duplicate products with identical SKUs. Stock exists on the INACTIVE duplicates while orders reference the ACTIVE products (which have zero stock). This causes \"Validate Picking\" jobs to fail because Odoo cannot reserve/pick items that have no stock on the active product.</p> <p>Root Cause (IDENTIFIED): 1. Oct 14, 2025: ~18,000 products were bulk imported OUTSIDE the Magento integration (no mappings created) 2. Oct 30 - Nov 14, 2025: Orders arrived from Magento for unmapped products 3. Integration behavior: Matches by Magento product ID, not SKU. With <code>auto_create_products_on_so=true</code>, it created NEW products with proper mappings instead of finding existing ones 4. Result: 18 products exist as duplicates - originals have stock but no mapping, duplicates have mapping but no stock 5. Consequence: Original products deactivated, stock trapped, orders fail</p>"},{"location":"ODOO-DUPLICATE-SKU-REMEDIATION/#part-1-immediate-order-resolution","title":"PART 1: Immediate Order Resolution","text":""},{"location":"ODOO-DUPLICATE-SKU-REMEDIATION/#task-11-mark-failed-queue-jobs-as-done","title":"Task 1.1: Mark Failed Queue Jobs as Done","text":"<p>Location: Odoo &gt; Queue &gt; Jobs (filter by State = Failed)</p> <p>Steps: 1. Navigate to Queue &gt; Jobs in Odoo 2. Filter: <code>State = Failed</code> AND <code>Name contains \"Validate Picking\"</code> 3. Select all 4 failed jobs (including order 100290210) 4. Click Action &gt; \"Set to Done\"</p> <p>Why: These are informational jobs that alert users to stock issues. They don't need to be requeued - they just need acknowledgment.</p> <p>Affected Jobs: - Job ID 1545341: Order 100290210 - \"No Magento shipments to apply\" - Job ID 1545318: Order 100290210 - \"remaining pickings are waiting for stock\" - (2 other similar jobs)</p>"},{"location":"ODOO-DUPLICATE-SKU-REMEDIATION/#task-12-resolve-order-100290210-picking","title":"Task 1.2: Resolve Order 100290210 Picking","text":"<p>Order Details: - Order: 100290210 (Odoo sale.order ID: 314341) - Pending Picking: TECH/OUT/285655 (state: confirmed) - Missing Item: \"Lagenda Mini Helium Digital Sizer Inflator\" (SKU: MG-09191, product_id: 20532) - Quantity Needed: 1 unit</p> <p>Option A - Transfer Stock (Recommended): 1. Go to Inventory &gt; Operations &gt; Inventory Adjustments 2. Create new adjustment for product 20532 (active MG-09191) 3. Add 1 unit to Stock location 4. Validate the adjustment 5. Go to picking TECH/OUT/285655 and click \"Check Availability\" 6. Process the picking</p> <p>Option B - Cancel Backorder: 1. If item won't be shipped, go to picking TECH/OUT/285655 2. Cancel the picking 3. Update the sale order accordingly 4. Communicate with customer about partial fulfillment</p>"},{"location":"ODOO-DUPLICATE-SKU-REMEDIATION/#part-2-inventory-stock-transfer","title":"PART 2: Inventory Stock Transfer","text":""},{"location":"ODOO-DUPLICATE-SKU-REMEDIATION/#alternative-fix-mappings-instead-of-transferring-stock","title":"ALTERNATIVE: Fix Mappings Instead of Transferring Stock","text":"<p>Instead of transferring stock to duplicate products, you could redirect Magento mappings to the ORIGINAL products (which have stock). This is cleaner if originals have order history.</p> <p>Steps: 1. Delete the duplicate products (20532, 20557, 20563, 20569, 20591, 20626, 20558) 2. Update mappings to point to original products 3. Reactivate original products</p> <p>SQL to redirect mappings (BACKUP FIRST): <pre><code>-- Example: Point MG-09191 mapping to original product 9218 instead of duplicate 20532\nUPDATE integration_product_product_mapping\nSET product_id = 9218, write_date = NOW()\nWHERE product_id = 20532 AND integration_id = 1;\n\n-- Reactivate original product\nUPDATE product_product SET active = true, write_date = NOW() WHERE id = 9218;\n\n-- Archive duplicate (don't delete - keep for audit)\nUPDATE product_product SET active = false, default_code = 'MG-09191-DUP-ARCHIVED', write_date = NOW() WHERE id = 20532;\n</code></pre></p> <p>WARNING: Only do this if duplicate products have NO order history. Check first: <pre><code>SELECT COUNT(*) FROM sale_order_line WHERE product_id IN (20532, 20557, 20563, 20569, 20591, 20626, 20558);\n</code></pre></p>"},{"location":"ODOO-DUPLICATE-SKU-REMEDIATION/#task-21-transfer-trapped-stock-from-inactive-products","title":"Task 2.1: Transfer Trapped Stock from Inactive Products","text":"<p>Products requiring stock transfer:</p> SKU FROM (Inactive) TO (Active) Qty to Transfer MG-09191 product_id: 9218 product_id: 20532 6 units MG-15781 product_id: 20569 Find active duplicate 6 units MG-15782 product_id: 20591 Find active duplicate 6 units MG-15784 product_id: 20626 Find active duplicate 8 units MG-18703 product_id: 20558 Find active duplicate 4 units <p>Steps for each SKU: 1. Go to Inventory &gt; Products, search by SKU 2. Identify the ACTIVE product variant 3. Go to Inventory &gt; Operations &gt; Inventory Adjustments 4. Create adjustment to ADD stock to the active product 5. Validate 6. Optionally: Create adjustment to REMOVE stock from inactive product (cleanup)</p> <p>SQL to find active product IDs for each SKU: <pre><code>SELECT pp.id, pp.default_code, pp.active\nFROM product_product pp\nWHERE pp.default_code IN ('MG-15781', 'MG-15782', 'MG-15784', 'MG-18703')\nORDER BY pp.default_code, pp.active DESC;\n</code></pre></p>"},{"location":"ODOO-DUPLICATE-SKU-REMEDIATION/#task-22-verify-stock-after-transfer","title":"Task 2.2: Verify Stock After Transfer","text":"<p>Verification Query (run in PostgreSQL): <pre><code>SELECT pp.default_code as sku, pp.id, pp.active,\n       COALESCE(SUM(sq.quantity) FILTER (WHERE sl.usage = 'internal'), 0) as stock\nFROM product_product pp\nLEFT JOIN stock_quant sq ON sq.product_id = pp.id\nLEFT JOIN stock_location sl ON sq.location_id = sl.id\nWHERE pp.default_code IN ('MG-09191', 'MG-15781', 'MG-15782', 'MG-15784', 'MG-18703')\nGROUP BY pp.id, pp.default_code, pp.active\nORDER BY pp.default_code, pp.active DESC;\n</code></pre></p> <p>Expected Result: Active products should have stock, inactive should have 0.</p>"},{"location":"ODOO-DUPLICATE-SKU-REMEDIATION/#part-3-duplicate-product-cleanup","title":"PART 3: Duplicate Product Cleanup","text":""},{"location":"ODOO-DUPLICATE-SKU-REMEDIATION/#task-31-merge-or-archive-duplicate-products","title":"Task 3.1: Merge or Archive Duplicate Products","text":"<p>For each duplicate SKU pair, decide:</p> <p>Option A - Merge Products (if both have history): 1. Export all sale order lines, purchase order lines, stock moves referencing the inactive product 2. Update references to point to active product 3. Archive (don't delete) the inactive product</p> <p>Option B - Keep Separate (if needed for historical records): 1. Rename inactive product SKU to add suffix (e.g., MG-09191-OLD) 2. This prevents future mapping conflicts</p> <p>List of all duplicates to review: <pre><code>MG-09191, MG-11788, MG-12046, MG-15781, MG-15782, MG-15784,\nMG-18703, MG-20344, MG-20476, MG-20483, MG-20489, MG-20490,\ncustomshippingrate_standard\n</code></pre></p>"},{"location":"ODOO-DUPLICATE-SKU-REMEDIATION/#task-32-update-magento-product-mappings","title":"Task 3.2: Update Magento Product Mappings","text":"<p>Location: Odoo &gt; Sales &gt; Integrations &gt; Hayden-Magento2 &gt; Product Mappings</p> <p>Steps: 1. Search for each affected SKU in the integration mappings 2. Verify the external Magento product ID maps to the ACTIVE Odoo product 3. If mapped to inactive product, update the mapping to active product 4. Test by triggering a product sync from Magento</p>"},{"location":"ODOO-DUPLICATE-SKU-REMEDIATION/#part-4-root-cause-prevention","title":"PART 4: Root Cause Prevention","text":""},{"location":"ODOO-DUPLICATE-SKU-REMEDIATION/#task-41-root-cause-identified","title":"Task 4.1: ROOT CAUSE IDENTIFIED \u2713","text":"<p>Timeline of Events: 1. Oct 12, 2025: Hayden-Magento2 integration was configured 2. Oct 14, 2025: ~18,000 products bulk imported into Odoo (by thomas@haydenagencies.com.au)    - Import was done OUTSIDE the integration (likely via Odoo's standard import or migration script)    - Products were created with SKUs but NO Magento mappings were created 3. Oct 30, 2025: First Magento product mappings created (integration starts syncing) 4. Nov 3-14, 2025: Orders arrive from Magento for products without mappings    - Integration looks for products by Magento product ID (not SKU)    - Cannot find mappings for 18 specific products    - <code>auto_create_products_on_so = true</code> setting causes creation of duplicate products WITH mappings 5. Later: Original products deactivated (seen as duplicates), but stock remained on them</p> <p>Root Cause: The Oct 14 bulk import bypassed the Magento integration, creating products in Odoo without the corresponding <code>integration_product_product_mapping</code> records. The integration matches by Magento product ID (<code>integration_product_product_external.code</code>), NOT by SKU.</p> <p>18 Products Affected (No Magento Mapping): <pre><code>-- Products created Oct 14 without mappings (includes our problem SKUs)\nSELECT pp.id, pp.default_code, pp.active\nFROM product_product pp\nLEFT JOIN integration_product_product_mapping ppm ON ppm.product_id = pp.id\nWHERE pp.create_date::date = '2025-10-14' AND ppm.id IS NULL;\n</code></pre> Result: MG-09191, MG-11617, MG-11618, MG-11710, MG-11711, MG-11712, MG-11788, MG-12046, MG-15781, MG-15782, MG-15784, MG-15833, MG-15836, MG-15837, MG-18703, MG-19901, and 2 blank SKUs.</p>"},{"location":"ODOO-DUPLICATE-SKU-REMEDIATION/#task-42-add-sku-uniqueness-constraint-optional","title":"Task 4.2: Add SKU Uniqueness Constraint (Optional)","text":"<p>Warning: Only do this after cleaning up duplicates!</p> <p>In Odoo: 1. Go to Settings &gt; Technical &gt; Database Structure &gt; Models 2. Find product.product model 3. Consider adding SQL constraint for unique active SKUs</p> <p>Or via SQL: <pre><code>-- First verify no active duplicates exist\nSELECT default_code, COUNT(*)\nFROM product_product\nWHERE active = true AND default_code IS NOT NULL\nGROUP BY default_code\nHAVING COUNT(*) &gt; 1;\n\n-- If clean, add partial unique index\nCREATE UNIQUE INDEX idx_product_sku_unique_active\nON product_product (default_code)\nWHERE active = true AND default_code IS NOT NULL AND default_code != '';\n</code></pre></p>"},{"location":"ODOO-DUPLICATE-SKU-REMEDIATION/#task-43-configure-integration-to-prevent-future-duplicates","title":"Task 4.3: Configure Integration to Prevent Future Duplicates","text":"<p>Current Configuration (Hayden-Magento2): <pre><code>auto_create_products_on_so = TRUE  &lt;-- This caused duplicates\nproduct_reference_id = 21\ntemplate_reference_id = 15\n</code></pre></p> <p>Option A - Disable Auto-Create (Recommended for stable catalogs): <pre><code>Location: Odoo &gt; Sales &gt; Integrations &gt; Hayden-Magento2 &gt; Settings\nSet \"Auto Create Products on Sales Order\" = False\n</code></pre> This prevents automatic product creation when orders contain unknown products. Orders will fail instead of creating duplicates.</p> <p>Option B - Keep Auto-Create but Add SKU Fallback Matching: The integration matches by Magento product ID, not SKU. If products are imported outside the integration: 1. ALWAYS use the integration's product import feature 2. OR manually create mappings after external imports</p> <p>For Future Imports - CRITICAL: Never bulk import products outside the Magento integration. Always use: - Odoo &gt; Sales &gt; Integrations &gt; Hayden-Magento2 &gt; Import Products - This creates proper mappings in <code>integration_product_product_mapping</code></p> <p>SQL to manually create missing mapping (if needed): <pre><code>-- Example: Link existing Odoo product to Magento product\nINSERT INTO integration_product_product_mapping\n  (integration_id, product_id, external_product_id, create_uid, write_uid, create_date, write_date)\nSELECT\n  1,                    -- Hayden-Magento2 integration_id\n  9218,                 -- Odoo product_product.id (original)\n  ppe.id,               -- external_product_id\n  1, 1, NOW(), NOW()\nFROM integration_product_product_external ppe\nWHERE ppe.external_reference = 'MG-09191'\n  AND ppe.integration_id = 1;\n</code></pre></p>"},{"location":"ODOO-DUPLICATE-SKU-REMEDIATION/#part-5-ongoing-monitoring","title":"PART 5: Ongoing Monitoring","text":""},{"location":"ODOO-DUPLICATE-SKU-REMEDIATION/#task-51-create-scheduled-check-for-duplicate-skus","title":"Task 5.1: Create Scheduled Check for Duplicate SKUs","text":"<p>Add to regular maintenance: Run this query weekly to catch new duplicates:</p> <pre><code>SELECT pp.default_code as sku, COUNT(*) as duplicates\nFROM product_product pp\nWHERE pp.default_code IS NOT NULL\n  AND pp.default_code != ''\n  AND pp.active = true\nGROUP BY pp.default_code\nHAVING COUNT(*) &gt; 1;\n</code></pre>"},{"location":"ODOO-DUPLICATE-SKU-REMEDIATION/#task-52-monitor-failed-picking-jobs","title":"Task 5.2: Monitor Failed Picking Jobs","text":"<p>Add alert for: Queue jobs with name containing \"Validate Picking\" and state = \"failed\"</p> <p>This catches stock availability issues early before they accumulate.</p>"},{"location":"ODOO-DUPLICATE-SKU-REMEDIATION/#quick-reference-database-access","title":"Quick Reference: Database Access","text":"<p>Connect to Odoo PostgreSQL: <pre><code>kubectl exec -n business-system odoo-pg-1 -- psql -U postgres -d prod\n</code></pre></p> <p>Key Tables: - <code>product_product</code> - Product variants (has SKU in default_code) - <code>stock_quant</code> - Current stock levels by location - <code>stock_move</code> - Stock movement history - <code>sale_order</code> / <code>sale_order_line</code> - Sales orders - <code>stock_picking</code> - Delivery orders/pickings - <code>queue_job</code> - Background job queue</p>"},{"location":"ODOO-DUPLICATE-SKU-REMEDIATION/#assignee-notes","title":"Assignee Notes","text":"<ul> <li>Priority: HIGH for Part 1 (order resolution) and Part 2 (stock transfer)</li> <li>Priority: MEDIUM for Part 3 (cleanup) and Part 4 (prevention)</li> <li>Estimated effort: 2-4 hours for immediate fixes, additional time for root cause investigation</li> </ul>"},{"location":"auntalma-magento-deployment/","title":"Magento 2.4.7 Kubernetes Deployment Plan","text":""},{"location":"auntalma-magento-deployment/#overview","title":"Overview","text":"<p>Deploy auntalma.com.au Magento 2.4.7-p7 store to the GitOps cluster using established FluxCD patterns.</p> <p>Strategy: Deploy dev environment first \u2192 verify everything works \u2192 migrate production data</p>"},{"location":"auntalma-magento-deployment/#architecture-summary","title":"Architecture Summary","text":"Component Solution Version Storage Magento App Custom Docker image + app-template 2.4.7-p7 - PHP-FPM ghcr.io/hayden-agencies/magento-auntalma 8.3 - Database MariaDB Operator (Galera HA) 10.6 100Gi ceph-block Cache/Sessions Redis (embedded Bitnami) 7.2 5Gi ceph-block Search Elasticsearch 7.17 + Algolia 7.17 50Gi ceph-block Message Queue RabbitMQ (Bitnami) 3.13 10Gi ceph-block Media Storage CephFS (RWX) - 100Gi ceph-filesystem Var Storage CephFS (RWX) - 50Gi ceph-filesystem <p>Note: Both Elasticsearch (backend catalog) and Algolia (frontend search) are configured in dev.</p>"},{"location":"auntalma-magento-deployment/#key-decisions","title":"Key Decisions","text":""},{"location":"auntalma-magento-deployment/#1-helm-chart-custom-app-template-like-odoo","title":"1. Helm Chart: Custom app-template (like Odoo)","text":"<ul> <li>Pre-built charts (Bitnami deprecated, PHOENIX-MEDIA) bundle dependencies - conflicts with cluster patterns</li> <li>Custom modules (21+ Rival namespace) require custom Docker image anyway</li> <li>Aligns with existing Odoo deployment pattern</li> </ul>"},{"location":"auntalma-magento-deployment/#2-database-mariadb-operator","title":"2. Database: MariaDB Operator","text":"<ul> <li>CNPG doesn't support MySQL; Magento requires MySQL/MariaDB</li> <li>Provides HA via Galera Cluster (3-node)</li> <li>Supports backups to GCS (like CNPG Barman)</li> <li>New operator in <code>database-system</code> namespace</li> </ul>"},{"location":"auntalma-magento-deployment/#3-storage-cephfs-for-media-rwx","title":"3. Storage: CephFS for Media (RWX)","text":"<ul> <li>Enables horizontal pod scaling</li> <li><code>ceph-block</code> for database and Redis (performance)</li> <li><code>ceph-filesystem</code> for shared media/var directories</li> </ul>"},{"location":"auntalma-magento-deployment/#directory-structure","title":"Directory Structure","text":"<pre><code>kubernetes/apps/base/\n\u251c\u2500\u2500 database-system/\n\u2502   \u251c\u2500\u2500 mariadb-operator/           # NEW: Operator + CRDs\n\u2502   \u2502   \u251c\u2500\u2500 crds/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ks.yaml\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 app/\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 helmrelease.yaml\n\u2502   \u2502   \u2502       \u251c\u2500\u2500 helmrepository.yaml\n\u2502   \u2502   \u2502       \u2514\u2500\u2500 kustomization.yaml\n\u2502   \u2502   \u2514\u2500\u2500 app/\n\u2502   \u2502       \u251c\u2500\u2500 ks.yaml\n\u2502   \u2502       \u251c\u2500\u2500 helmrelease.yaml\n\u2502   \u2502       \u2514\u2500\u2500 kustomization.yaml\n\u2502\n\u251c\u2500\u2500 business-system/\n\u2502   \u251c\u2500\u2500 auntalma/                   # NEW: Magento for auntalma.com.au\n\u2502   \u2502   \u251c\u2500\u2500 ks.yaml\n\u2502   \u2502   \u2514\u2500\u2500 app/\n\u2502   \u2502       \u251c\u2500\u2500 mariadb-cluster.yaml\n\u2502   \u2502       \u251c\u2500\u2500 helmrelease.yaml\n\u2502   \u2502       \u251c\u2500\u2500 ocirepository.yaml\n\u2502   \u2502       \u251c\u2500\u2500 configmap-nginx.yaml\n\u2502   \u2502       \u251c\u2500\u2500 configmap-php.yaml\n\u2502   \u2502       \u251c\u2500\u2500 pvc-media.yaml\n\u2502   \u2502       \u251c\u2500\u2500 pvc-var.yaml\n\u2502   \u2502       \u251c\u2500\u2500 httproute.yaml\n\u2502   \u2502       \u251c\u2500\u2500 podmonitor.yaml\n\u2502   \u2502       \u251c\u2500\u2500 prometheusrule.yaml\n\u2502   \u2502       \u251c\u2500\u2500 externalsecret-ghcr.yaml\n\u2502   \u2502       \u251c\u2500\u2500 secret-magento-app.enc.age.yaml\n\u2502   \u2502       \u251c\u2500\u2500 secret-mariadb-app.enc.age.yaml\n\u2502   \u2502       \u251c\u2500\u2500 secret-mariadb-superuser.enc.age.yaml\n\u2502   \u2502       \u251c\u2500\u2500 replicationsource.yaml\n\u2502   \u2502       \u2514\u2500\u2500 kustomization.yaml\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 auntalma-elasticsearch/     # NEW: Elasticsearch 7.17\n\u2502   \u2502   \u251c\u2500\u2500 ks.yaml\n\u2502   \u2502   \u2514\u2500\u2500 app/\n\u2502   \u2502       \u251c\u2500\u2500 helmrelease.yaml\n\u2502   \u2502       \u251c\u2500\u2500 ocirepository.yaml\n\u2502   \u2502       \u2514\u2500\u2500 kustomization.yaml\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 auntalma-rabbitmq/          # NEW: RabbitMQ 3.13\n\u2502       \u251c\u2500\u2500 ks.yaml\n\u2502       \u2514\u2500\u2500 app/\n\u2502           \u251c\u2500\u2500 helmrelease.yaml\n\u2502           \u251c\u2500\u2500 ocirepository.yaml\n\u2502           \u251c\u2500\u2500 secret.enc.age.yaml\n\u2502           \u2514\u2500\u2500 kustomization.yaml\n\nterraform/cloudflare/\n\u2514\u2500\u2500 auntalma.tf                     # UPDATE: Add web DNS records\n</code></pre>"},{"location":"auntalma-magento-deployment/#dependency-chain","title":"Dependency Chain","text":"<pre><code>1. mariadb-operator-crds\n   \u2514\u2500\u2500 2. mariadb-operator\n       \u2514\u2500\u2500 3. auntalma (creates MariaDB cluster)\n\n1. rook-ceph-cluster (existing)\n   \u251c\u2500\u2500 2. auntalma-elasticsearch\n   \u251c\u2500\u2500 2. auntalma-rabbitmq\n   \u2514\u2500\u2500 2. auntalma (PVCs)\n\n1. onepassword (existing) \u2192 2. auntalma (ExternalSecrets)\n1. cilium-gateway-api (existing) \u2192 2. auntalma (HTTPRoute)\n</code></pre>"},{"location":"auntalma-magento-deployment/#resource-requirements","title":"Resource Requirements","text":"Component CPU Request Memory Request Memory Limit Replicas Auntalma PHP-FPM 500m 2Gi 8Gi 2 Auntalma Nginx 100m 128Mi 512Mi 2 (sidecar) Auntalma Cron 100m 512Mi 2Gi CronJob MariaDB Galera 500m 2Gi 8Gi 3 Redis 100m 256Mi 1Gi 1 Elasticsearch 250m 1Gi 2Gi 1 RabbitMQ 100m 256Mi 1Gi 1 <p>Total: ~3 CPU cores, ~11Gi memory requested, ~315Gi storage</p>"},{"location":"auntalma-magento-deployment/#implementation-strategy","title":"Implementation Strategy","text":""},{"location":"auntalma-magento-deployment/#stage-1-dev-environment-on-k8s","title":"Stage 1: Dev Environment on K8s","text":""},{"location":"auntalma-magento-deployment/#phase-1-infrastructure","title":"Phase 1: Infrastructure","text":"<ol> <li>Deploy MariaDB Operator to database-system</li> <li>Deploy Elasticsearch and RabbitMQ to business-system</li> <li>Create MariaDB Galera cluster for auntalma</li> <li>Verify all services healthy</li> </ol>"},{"location":"auntalma-magento-deployment/#phase-2-docker-image-build","title":"Phase 2: Docker Image Build","text":"<ol> <li>Create Dockerfile based on dev setup (PHP 8.3-FPM + Nginx)</li> <li>Include all Rival custom modules + composer dependencies</li> <li>Set up GitHub Actions workflow \u2192 push to GHCR</li> <li>Configure Renovate for digest tracking</li> </ol>"},{"location":"auntalma-magento-deployment/#phase-3-deploy-dev-data","title":"Phase 3: Deploy Dev Data","text":"<ol> <li>Import dev database to K8s MariaDB</li> <li>Copy dev media files to CephFS PVC</li> <li>Deploy Magento HelmRelease</li> <li>Update <code>core_config_data</code> for cluster URLs</li> <li>Reindex Elasticsearch</li> <li>Verify storefront + admin functionality</li> </ol>"},{"location":"auntalma-magento-deployment/#phase-4-validate","title":"Phase 4: Validate","text":"<ol> <li>Test checkout flow (Stripe/eWay)</li> <li>Test Algolia search</li> <li>Test admin operations</li> <li>Verify cron jobs running</li> <li>Check Prometheus metrics</li> </ol>"},{"location":"auntalma-magento-deployment/#stage-2-production-migration-later","title":"Stage 2: Production Migration (Later)","text":"<p>Once dev is validated: 1. Take production database backup 2. Sync media files from production server 3. Import production data to K8s 4. Update DNS (auntalma.com.au \u2192 cluster) 5. Monitor and verify</p>"},{"location":"auntalma-magento-deployment/#build-pipeline","title":"Build Pipeline","text":"<p>Custom Docker image built via GitHub Actions: - Base: PHP 8.3-FPM + Nginx - Includes: All 21+ Rival custom modules - Includes: Composer dependencies (Stripe, Amasty, Fooman, etc.) - Push to: ghcr.io/hayden-agencies/magento - Renovate tracks digest updates</p>"},{"location":"auntalma-magento-deployment/#verification-checklist","title":"Verification Checklist","text":"<ol> <li>All pods in <code>business-system</code> namespace running</li> <li>MariaDB Galera cluster shows 3/3 nodes synced (<code>SHOW STATUS LIKE 'wsrep_cluster_size'</code>)</li> <li><code>curl https://auntalma.haydenagencies.com.au/health_check.php</code> returns 200</li> <li>Admin login works at <code>/admin_hayden/</code></li> <li>Product search returns results (Elasticsearch reindex complete)</li> <li>Algolia autocomplete working on frontend</li> <li>Test order placement (RabbitMQ queue processing)</li> <li>Cron jobs executing (<code>bin/magento cron:run</code> logs)</li> <li>Prometheus metrics appearing in Grafana</li> <li>VolSync backup completing successfully</li> </ol>"},{"location":"auntalma-magento-deployment/#dns-configuration-terraform","title":"DNS Configuration (Terraform)","text":"<p>Add to <code>terraform/cloudflare/auntalma.tf</code>: <pre><code># Web traffic via Cloudflare Tunnel\nresource \"cloudflare_record\" \"auntalma_web\" {\n  zone_id = local.auntalma_zone_id\n  name    = \"@\"\n  content = \"external.haydenagencies.com.au\"\n  type    = \"CNAME\"\n  proxied = true\n  comment = \"Magento store via K8s cluster\"\n}\n\nresource \"cloudflare_record\" \"auntalma_www\" {\n  zone_id = local.auntalma_zone_id\n  name    = \"www\"\n  content = \"external.haydenagencies.com.au\"\n  type    = \"CNAME\"\n  proxied = true\n  comment = \"Magento store www redirect\"\n}\n</code></pre></p>"},{"location":"auntalma-magento-deployment/#decisions-made","title":"Decisions Made","text":"Decision Choice Domain auntalma.com.au Database MariaDB Galera 3-node HA Search Elasticsearch 7.17 + Algolia Media Storage 100Gi (under 50GB needed) Approach Dev environment first, then production"},{"location":"auntalma-magento-deployment/#files-to-create-summary","title":"Files to Create (Summary)","text":""},{"location":"auntalma-magento-deployment/#database-system-mariadb-operator","title":"database-system (MariaDB Operator)","text":"<ul> <li><code>mariadb-operator/crds/ks.yaml</code></li> <li><code>mariadb-operator/crds/app/helmrelease.yaml</code></li> <li><code>mariadb-operator/crds/app/helmrepository.yaml</code></li> <li><code>mariadb-operator/crds/app/kustomization.yaml</code></li> <li><code>mariadb-operator/app/ks.yaml</code></li> <li><code>mariadb-operator/app/helmrelease.yaml</code></li> <li><code>mariadb-operator/app/kustomization.yaml</code></li> </ul>"},{"location":"auntalma-magento-deployment/#business-system-auntalma","title":"business-system (Auntalma)","text":"<ul> <li><code>auntalma/ks.yaml</code></li> <li><code>auntalma/app/mariadb-cluster.yaml</code></li> <li><code>auntalma/app/helmrelease.yaml</code></li> <li><code>auntalma/app/ocirepository.yaml</code></li> <li><code>auntalma/app/configmap-nginx.yaml</code></li> <li><code>auntalma/app/configmap-php.yaml</code></li> <li><code>auntalma/app/pvc-media.yaml</code></li> <li><code>auntalma/app/pvc-var.yaml</code></li> <li><code>auntalma/app/httproute.yaml</code></li> <li><code>auntalma/app/secret-*.enc.age.yaml</code> (3 files)</li> <li><code>auntalma/app/kustomization.yaml</code></li> <li><code>auntalma-elasticsearch/ks.yaml</code></li> <li><code>auntalma-elasticsearch/app/helmrelease.yaml</code></li> <li><code>auntalma-elasticsearch/app/ocirepository.yaml</code></li> <li><code>auntalma-elasticsearch/app/kustomization.yaml</code></li> <li><code>auntalma-rabbitmq/ks.yaml</code></li> <li><code>auntalma-rabbitmq/app/helmrelease.yaml</code></li> <li><code>auntalma-rabbitmq/app/ocirepository.yaml</code></li> <li><code>auntalma-rabbitmq/app/secret.enc.age.yaml</code></li> <li><code>auntalma-rabbitmq/app/kustomization.yaml</code></li> </ul>"},{"location":"auntalma-magento-deployment/#terraform","title":"Terraform","text":"<ul> <li>Update <code>terraform/cloudflare/auntalma.tf</code> with web records</li> </ul>"},{"location":"auntalma-magento-deployment/#external-magento-source-repo","title":"External (Magento Source Repo)","text":"<ul> <li>Dockerfile for custom image</li> <li>GitHub Actions workflow for GHCR builds</li> </ul>"},{"location":"chatwoot-notifier-plan/","title":"Chatwoot Notifier - Implementation Plan","text":""},{"location":"chatwoot-notifier-plan/#overview","title":"Overview","text":"<p>A sidecar service that bridges Chatwoot and Odoo to provide SLA breach notifications and cross-platform alerts.</p>"},{"location":"chatwoot-notifier-plan/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                           CLUSTER (business-system)                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502     Chatwoot     \u2502                    \u2502    chatwoot-notifier     \u2502  \u2502\n\u2502  \u2502                  \u2502   POST /webhook    \u2502                          \u2502  \u2502\n\u2502  \u2502  conversation_   \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6  \u2502  1. Receive webhook      \u2502  \u2502\n\u2502  \u2502  created/updated \u2502                    \u2502  2. Check SLA status     \u2502  \u2502\n\u2502  \u2502                  \u2502 \u25c0\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502  3. Evaluate breach      \u2502  \u2502\n\u2502  \u2502                  \u2502   GET /api/v1/...  \u2502  4. Send notifications   \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502         :3000                                        \u2502 :5000           \u2502\n\u2502                                                      \u2502                 \u2502\n\u2502                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n\u2502                          \u2502                           \u2502        \u2502        \u2502\n\u2502                          \u25bc                           \u25bc        \u25bc        \u2502\n\u2502                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502                   \u2502    Odoo     \u2502            \u2502   SMTP    \u2502 \u2502Slack \u2502   \u2502\n\u2502                   \u2502             \u2502            \u2502  Server   \u2502 \u2502 API  \u2502   \u2502\n\u2502                   \u2502 XML-RPC API \u2502            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                   \u2502 :8071       \u2502                                      \u2502\n\u2502                   \u2502             \u2502                                      \u2502\n\u2502                   \u2502 \u2022 Activity  \u2502                                      \u2502\n\u2502                   \u2502 \u2022 Chatter   \u2502                                      \u2502\n\u2502                   \u2502 \u2022 Bus notif \u2502                                      \u2502\n\u2502                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                      \u2502\n\u2502                                                                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"chatwoot-notifier-plan/#components","title":"Components","text":""},{"location":"chatwoot-notifier-plan/#1-webhook-receiver","title":"1. Webhook Receiver","text":"<ul> <li>Flask/FastAPI endpoint receiving Chatwoot webhooks</li> <li>Events: <code>conversation_created</code>, <code>conversation_updated</code>, <code>message_created</code></li> <li>Validates webhook signature (if configured)</li> </ul>"},{"location":"chatwoot-notifier-plan/#2-sla-monitor","title":"2. SLA Monitor","text":"<ul> <li>Polls Chatwoot API for conversation SLA status</li> <li>Tracks <code>applied_sla</code> and <code>sla_events</code> from conversation details</li> <li>Calculates time-to-breach for FRT/NRT thresholds</li> </ul>"},{"location":"chatwoot-notifier-plan/#3-notification-dispatcher","title":"3. Notification Dispatcher","text":"<ul> <li>Email via SMTP (existing cluster SMTP config)</li> <li>Odoo activities via XML-RPC</li> <li>Optional: Slack webhook</li> </ul>"},{"location":"chatwoot-notifier-plan/#chatwoot-integration","title":"Chatwoot Integration","text":""},{"location":"chatwoot-notifier-plan/#webhook-events-to-subscribe","title":"Webhook Events to Subscribe","text":"Event Use Case <code>conversation_created</code> Start SLA tracking <code>conversation_updated</code> Check for SLA changes, assignee changes <code>message_created</code> Reset NRT timer on customer message"},{"location":"chatwoot-notifier-plan/#webhook-payload-conversation_updated","title":"Webhook Payload (conversation_updated)","text":"<pre><code>{\n  \"event\": \"conversation_updated\",\n  \"id\": 123,\n  \"account\": {\n    \"id\": 1,\n    \"name\": \"Hayden Agencies\"\n  },\n  \"changed_attributes\": {\n    \"sla_policy_id\": {\n      \"previous_value\": null,\n      \"current_value\": 1\n    }\n  },\n  \"meta\": {\n    \"assignee\": {\n      \"id\": 2,\n      \"name\": \"Anita Karisson\",\n      \"email\": \"anita@haydenagencies.com.au\"\n    },\n    \"team\": {\n      \"id\": 1,\n      \"name\": \"customer service\"\n    }\n  },\n  \"sla_policy_id\": 1\n}\n</code></pre>"},{"location":"chatwoot-notifier-plan/#api-endpoints-required","title":"API Endpoints Required","text":"Endpoint Purpose <code>GET /api/v1/accounts/{id}/conversations/{id}</code> Get full conversation with <code>applied_sla</code>, <code>sla_events</code> <code>GET /api/v1/accounts/{id}/sla_policies</code> List SLA policies with thresholds <code>GET /api/v1/accounts/{id}/agents</code> Get agent list for user mapping"},{"location":"chatwoot-notifier-plan/#api-response-conversation-details","title":"API Response - Conversation Details","text":"<pre><code>{\n  \"id\": 123,\n  \"status\": \"open\",\n  \"assignee_id\": 2,\n  \"team_id\": 1,\n  \"sla_policy_id\": 1,\n  \"applied_sla\": {\n    \"id\": 1,\n    \"sla_status\": \"active\",\n    \"created_at\": \"2026-02-04T09:00:00Z\"\n  },\n  \"sla_events\": [\n    {\n      \"event_type\": \"frt_breach\",\n      \"created_at\": \"2026-02-04T09:05:30Z\",\n      \"meta\": {}\n    }\n  ]\n}\n</code></pre>"},{"location":"chatwoot-notifier-plan/#sla-event-types","title":"SLA Event Types","text":"Event Type Meaning <code>frt</code> First Response Time tracking started <code>frt_breach</code> FRT threshold exceeded <code>nrt</code> Next Response Time tracking started <code>nrt_breach</code> NRT threshold exceeded <code>rt</code> Resolution Time tracking started <code>rt_breach</code> Resolution Time exceeded"},{"location":"chatwoot-notifier-plan/#odoo-integration","title":"Odoo Integration","text":""},{"location":"chatwoot-notifier-plan/#connection-details","title":"Connection Details","text":"<pre><code>host: odoo.business-system.svc.cluster.local\nport: 8071  # XML-RPC port\ndatabase: odoo  # from secret odoo-pg-app\n</code></pre>"},{"location":"chatwoot-notifier-plan/#authentication","title":"Authentication","text":"<pre><code>import xmlrpc.client\n\nurl = \"http://odoo.business-system.svc.cluster.local:8071\"\ndb = \"odoo\"\nusername = \"admin\"  # or API user\npassword = \"...\"    # from secret\n\n# Authenticate\ncommon = xmlrpc.client.ServerProxy(f\"{url}/xmlrpc/2/common\")\nuid = common.authenticate(db, username, password, {})\n\n# Object proxy for API calls\nmodels = xmlrpc.client.ServerProxy(f\"{url}/xmlrpc/2/object\")\n</code></pre>"},{"location":"chatwoot-notifier-plan/#creating-activities","title":"Creating Activities","text":"<pre><code># Get model ID for res.partner (or res.users)\nmodel_id = models.execute_kw(\n    db, uid, password,\n    'ir.model', 'search',\n    [[['model', '=', 'res.partner']]]\n)[0]\n\n# Find Odoo user by email\nuser_ids = models.execute_kw(\n    db, uid, password,\n    'res.users', 'search',\n    [[['email', '=', 'skye@haydenagencies.com.au']]]\n)\n\n# Create activity (appears in user's activity panel)\nactivity_id = models.execute_kw(\n    db, uid, password,\n    'mail.activity', 'create',\n    [{\n        'activity_type_id': 4,  # To-Do type\n        'summary': 'Chatwoot SLA Breach Alert',\n        'note': '&lt;p&gt;Conversation #123 has breached FRT SLA (5 min)&lt;/p&gt;',\n        'date_deadline': '2026-02-04',\n        'user_id': user_ids[0],\n        'res_model_id': model_id,\n        'res_id': user_ids[0],  # Link to user's partner record\n    }]\n)\n</code></pre>"},{"location":"chatwoot-notifier-plan/#sending-notifications-chatter","title":"Sending Notifications (Chatter)","text":"<pre><code># Post message to a record (appears in chatter)\nmodels.execute_kw(\n    db, uid, password,\n    'res.users', 'message_post',\n    [user_ids],\n    {\n        'body': '&lt;p&gt;&lt;strong&gt;SLA Breach Alert&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Conversation #123 breached FRT&lt;/p&gt;',\n        'message_type': 'notification',\n        'subtype_xmlid': 'mail.mt_note',\n    }\n)\n</code></pre>"},{"location":"chatwoot-notifier-plan/#bus-notification-popup-toast","title":"Bus Notification (Popup Toast)","text":"<pre><code># Send instant notification (requires longpolling connection)\nmodels.execute_kw(\n    db, uid, password,\n    'bus.bus', '_sendone',\n    [\n        f'res.partner_{partner_id}',  # Channel\n        'simple_notification',\n        {\n            'title': 'SLA Breach',\n            'message': 'Conversation #123 breached FRT threshold',\n            'sticky': True,\n            'type': 'warning',\n        }\n    ]\n)\n</code></pre>"},{"location":"chatwoot-notifier-plan/#user-mapping","title":"User Mapping","text":""},{"location":"chatwoot-notifier-plan/#strategy-email-based-matching","title":"Strategy: Email-based matching","text":"<p>Both Chatwoot and Odoo use email as unique identifier.</p> <pre><code># ConfigMap: chatwoot-notifier-config\nuser_mappings:\n  # Chatwoot email -&gt; notification preferences\n  anita@haydenagencies.com.au:\n    odoo_notify: true\n    email_notify: true\n    teams: [customer_service]\n\n  dearne@haydenagencies.com.au:\n    odoo_notify: true\n    email_notify: true\n    teams: [customer_service]\n\n  skye@haydenagencies.com.au:\n    odoo_notify: true\n    email_notify: true\n    teams: [management]\n\n  ashleigh@haydenagencies.com.au:\n    odoo_notify: true\n    email_notify: true\n    teams: [management]\n\n  mehreen@haydenagencies.com.au:\n    odoo_notify: true\n    email_notify: true\n    teams: [management]\n\n  thomas@haydenagencies.com.au:\n    odoo_notify: false\n    email_notify: false\n    teams: [owner]\n\n  isabelle@haydenagencies.com.au:\n    odoo_notify: false\n    email_notify: false\n    teams: [owner]\n</code></pre>"},{"location":"chatwoot-notifier-plan/#notification-logic","title":"Notification Logic","text":""},{"location":"chatwoot-notifier-plan/#trigger-conditions","title":"Trigger Conditions","text":"<pre><code>NOTIFICATION_RULES = [\n    {\n        \"name\": \"SLA FRT Breach\",\n        \"condition\": lambda conv: \"frt_breach\" in [e[\"event_type\"] for e in conv.get(\"sla_events\", [])],\n        \"notify_teams\": [\"management\"],\n        \"channels\": [\"email\", \"odoo_activity\"],\n        \"message\": \"Conversation #{id} breached First Response Time SLA ({threshold})\"\n    },\n    {\n        \"name\": \"SLA NRT Breach\",\n        \"condition\": lambda conv: \"nrt_breach\" in [e[\"event_type\"] for e in conv.get(\"sla_events\", [])],\n        \"notify_teams\": [\"management\"],\n        \"channels\": [\"email\", \"odoo_activity\"],\n        \"message\": \"Conversation #{id} breached Next Response Time SLA ({threshold})\"\n    },\n    {\n        \"name\": \"Unassigned Conversation Warning\",\n        \"condition\": lambda conv: conv.get(\"assignee_id\") is None and conv.get(\"status\") == \"open\",\n        \"notify_teams\": [\"management\"],\n        \"channels\": [\"odoo_activity\"],\n        \"message\": \"Conversation #{id} is unassigned\"\n    },\n]\n</code></pre>"},{"location":"chatwoot-notifier-plan/#deduplication","title":"Deduplication","text":"<p>Track sent notifications to avoid spam:</p> <pre><code># Redis or in-memory cache\nnotification_cache = {}\n\ndef should_notify(conversation_id: int, event_type: str) -&gt; bool:\n    key = f\"{conversation_id}:{event_type}\"\n    if key in notification_cache:\n        return False\n    notification_cache[key] = datetime.now()\n    return True\n</code></pre>"},{"location":"chatwoot-notifier-plan/#kubernetes-deployment","title":"Kubernetes Deployment","text":""},{"location":"chatwoot-notifier-plan/#directory-structure","title":"Directory Structure","text":"<pre><code>kubernetes/apps/base/business-system/chatwoot-notifier/\n\u251c\u2500\u2500 ks.yaml                          # Flux Kustomization\n\u2514\u2500\u2500 app/\n    \u251c\u2500\u2500 kustomization.yaml\n    \u251c\u2500\u2500 deployment.yaml              # Or helmrelease.yaml with app-template\n    \u251c\u2500\u2500 service.yaml\n    \u251c\u2500\u2500 configmap.yaml               # User mappings, notification rules\n    \u251c\u2500\u2500 secret.enc.age.yaml          # API keys, credentials\n    \u2514\u2500\u2500 networkpolicy.yaml           # Allow Chatwoot, Odoo, SMTP\n</code></pre>"},{"location":"chatwoot-notifier-plan/#deployment-manifest","title":"Deployment Manifest","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: chatwoot-notifier\n  namespace: business-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: chatwoot-notifier\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: chatwoot-notifier\n    spec:\n      containers:\n        - name: notifier\n          image: ghcr.io/hayden-agencies/chatwoot-notifier:latest\n          ports:\n            - containerPort: 5000\n          env:\n            - name: CHATWOOT_URL\n              value: \"http://chatwoot-web.business-system.svc.cluster.local:3000\"\n            - name: CHATWOOT_API_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: chatwoot-notifier\n                  key: chatwoot-api-token\n            - name: ODOO_URL\n              value: \"http://odoo.business-system.svc.cluster.local:8071\"\n            - name: ODOO_DB\n              valueFrom:\n                secretKeyRef:\n                  name: odoo-pg-app\n                  key: dbname\n            - name: ODOO_USER\n              valueFrom:\n                secretKeyRef:\n                  name: chatwoot-notifier\n                  key: odoo-user\n            - name: ODOO_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: chatwoot-notifier\n                  key: odoo-password\n            - name: SMTP_HOST\n              value: \"smtp.gmail.com\"\n            - name: SMTP_PORT\n              value: \"587\"\n          volumeMounts:\n            - name: config\n              mountPath: /app/config\n      volumes:\n        - name: config\n          configMap:\n            name: chatwoot-notifier-config\n</code></pre>"},{"location":"chatwoot-notifier-plan/#networkpolicy","title":"NetworkPolicy","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: chatwoot-notifier\n  namespace: business-system\nspec:\n  podSelector:\n    matchLabels:\n      app.kubernetes.io/name: chatwoot-notifier\n  policyTypes:\n    - Ingress\n    - Egress\n  ingress:\n    # Allow webhooks from Chatwoot\n    - from:\n        - podSelector:\n            matchLabels:\n              app.kubernetes.io/name: chatwoot\n      ports:\n        - port: 5000\n  egress:\n    # DNS\n    - to:\n        - namespaceSelector:\n            matchLabels:\n              kubernetes.io/metadata.name: kube-system\n      ports:\n        - port: 53\n          protocol: UDP\n    # Chatwoot API\n    - to:\n        - podSelector:\n            matchLabels:\n              app.kubernetes.io/name: chatwoot\n      ports:\n        - port: 3000\n    # Odoo XML-RPC\n    - to:\n        - podSelector:\n            matchLabels:\n              app.kubernetes.io/name: odoo\n      ports:\n        - port: 8071\n    # SMTP\n    - to:\n        - ipBlock:\n            cidr: 0.0.0.0/0\n      ports:\n        - port: 587\n        - port: 465\n</code></pre>"},{"location":"chatwoot-notifier-plan/#application-code-structure","title":"Application Code Structure","text":"<pre><code>chatwoot-notifier/\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 app/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 main.py              # Flask app entry point\n\u2502   \u251c\u2500\u2500 config.py            # Configuration management\n\u2502   \u251c\u2500\u2500 webhooks/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 chatwoot.py      # Webhook handlers\n\u2502   \u251c\u2500\u2500 clients/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 chatwoot.py      # Chatwoot API client\n\u2502   \u2502   \u2514\u2500\u2500 odoo.py          # Odoo XML-RPC client\n\u2502   \u251c\u2500\u2500 notifications/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 dispatcher.py    # Notification routing\n\u2502   \u2502   \u251c\u2500\u2500 email.py         # Email sender\n\u2502   \u2502   \u2514\u2500\u2500 odoo.py          # Odoo notification sender\n\u2502   \u2514\u2500\u2500 sla/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 monitor.py       # SLA breach detection\n\u2514\u2500\u2500 tests/\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"chatwoot-notifier-plan/#main-application-mainpy","title":"Main Application (main.py)","text":"<pre><code>from flask import Flask, request, jsonify\nfrom app.webhooks.chatwoot import handle_webhook\nfrom app.config import Config\n\napp = Flask(__name__)\nconfig = Config()\n\n@app.route('/health', methods=['GET'])\ndef health():\n    return jsonify({\"status\": \"healthy\"})\n\n@app.route('/webhook/chatwoot', methods=['POST'])\ndef chatwoot_webhook():\n    payload = request.json\n    result = handle_webhook(payload, config)\n    return jsonify(result)\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)\n</code></pre>"},{"location":"chatwoot-notifier-plan/#chatwoot-webhook-handler","title":"Chatwoot Webhook Handler","text":"<pre><code>from app.clients.chatwoot import ChatwootClient\nfrom app.notifications.dispatcher import NotificationDispatcher\nfrom app.sla.monitor import check_sla_breach\n\ndef handle_webhook(payload: dict, config) -&gt; dict:\n    event = payload.get('event')\n\n    if event == 'conversation_created':\n        # New conversation - start monitoring\n        return {\"status\": \"received\", \"action\": \"monitoring_started\"}\n\n    elif event == 'conversation_updated':\n        conversation_id = payload.get('id')\n        account_id = payload.get('account', {}).get('id')\n\n        # Fetch full conversation with SLA details\n        client = ChatwootClient(config.chatwoot_url, config.chatwoot_token)\n        conversation = client.get_conversation(account_id, conversation_id)\n\n        # Check for SLA breaches\n        breaches = check_sla_breach(conversation)\n\n        if breaches:\n            dispatcher = NotificationDispatcher(config)\n            for breach in breaches:\n                dispatcher.notify(breach, conversation)\n\n        return {\"status\": \"processed\", \"breaches\": len(breaches)}\n\n    return {\"status\": \"ignored\"}\n</code></pre>"},{"location":"chatwoot-notifier-plan/#odoo-notification-client","title":"Odoo Notification Client","text":"<pre><code>import xmlrpc.client\nfrom typing import Optional\n\nclass OdooClient:\n    def __init__(self, url: str, db: str, username: str, password: str):\n        self.url = url\n        self.db = db\n        self.username = username\n        self.password = password\n        self._uid = None\n        self._models = None\n\n    def connect(self):\n        common = xmlrpc.client.ServerProxy(f\"{self.url}/xmlrpc/2/common\")\n        self._uid = common.authenticate(self.db, self.username, self.password, {})\n        self._models = xmlrpc.client.ServerProxy(f\"{self.url}/xmlrpc/2/object\")\n        return self._uid is not None\n\n    def get_user_by_email(self, email: str) -&gt; Optional[int]:\n        user_ids = self._models.execute_kw(\n            self.db, self._uid, self.password,\n            'res.users', 'search',\n            [[['email', '=', email]]]\n        )\n        return user_ids[0] if user_ids else None\n\n    def create_activity(self, user_id: int, summary: str, note: str):\n        # Get partner model ID\n        model_id = self._models.execute_kw(\n            self.db, self._uid, self.password,\n            'ir.model', 'search',\n            [[['model', '=', 'res.users']]]\n        )[0]\n\n        return self._models.execute_kw(\n            self.db, self._uid, self.password,\n            'mail.activity', 'create',\n            [{\n                'activity_type_id': 4,  # To-Do\n                'summary': summary,\n                'note': note,\n                'date_deadline': datetime.now().strftime('%Y-%m-%d'),\n                'user_id': user_id,\n                'res_model_id': model_id,\n                'res_id': user_id,\n            }]\n        )\n\n    def send_notification(self, user_id: int, title: str, message: str):\n        # Get partner ID for user\n        partner_id = self._models.execute_kw(\n            self.db, self._uid, self.password,\n            'res.users', 'read',\n            [user_id],\n            {'fields': ['partner_id']}\n        )[0]['partner_id'][0]\n\n        # Send bus notification\n        self._models.execute_kw(\n            self.db, self._uid, self.password,\n            'bus.bus', '_sendone',\n            [\n                f'res.partner_{partner_id}',\n                'simple_notification',\n                {\n                    'title': title,\n                    'message': message,\n                    'sticky': True,\n                    'type': 'warning',\n                }\n            ]\n        )\n</code></pre>"},{"location":"chatwoot-notifier-plan/#chatwoot-webhook-configuration","title":"Chatwoot Webhook Configuration","text":"<p>After deploying the notifier, register the webhook in Chatwoot:</p> <pre><code># Via Rails console\nkubectl exec -n business-system chatwoot-web-xxx -- bundle exec rails runner '\naccount = Account.find(1)\nWebhook.create!(\n  account_id: account.id,\n  url: \"http://chatwoot-notifier.business-system.svc.cluster.local:5000/webhook/chatwoot\",\n  subscriptions: [\"conversation_created\", \"conversation_updated\", \"message_created\"]\n)\nputs \"Webhook created!\"\n'\n</code></pre> <p>Or via API:</p> <pre><code>curl -X POST \"https://chat.haydenagencies.com.au/api/v1/accounts/1/webhooks\" \\\n  -H \"api_access_token: YOUR_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"url\": \"http://chatwoot-notifier.business-system.svc.cluster.local:5000/webhook/chatwoot\",\n    \"subscriptions\": [\"conversation_created\", \"conversation_updated\", \"message_created\"]\n  }'\n</code></pre>"},{"location":"chatwoot-notifier-plan/#secrets-required","title":"Secrets Required","text":""},{"location":"chatwoot-notifier-plan/#chatwoot-notifier-secret","title":"chatwoot-notifier secret","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: chatwoot-notifier\n  namespace: business-system\ntype: Opaque\nstringData:\n  chatwoot-api-token: \"...\"      # From Chatwoot admin settings\n  odoo-user: \"admin\"             # Or dedicated API user\n  odoo-password: \"...\"           # Odoo user password\n  smtp-username: \"...\"           # If different from Chatwoot SMTP\n  smtp-password: \"...\"\n</code></pre>"},{"location":"chatwoot-notifier-plan/#implementation-phases","title":"Implementation Phases","text":""},{"location":"chatwoot-notifier-plan/#phase-1-core-service","title":"Phase 1: Core Service","text":"<ul> <li> Create Python Flask application</li> <li> Implement webhook receiver</li> <li> Implement Chatwoot API client</li> <li> Basic SLA breach detection</li> </ul>"},{"location":"chatwoot-notifier-plan/#phase-2-odoo-integration","title":"Phase 2: Odoo Integration","text":"<ul> <li> Implement Odoo XML-RPC client</li> <li> Activity creation</li> <li> User email mapping</li> </ul>"},{"location":"chatwoot-notifier-plan/#phase-3-kubernetes-deployment","title":"Phase 3: Kubernetes Deployment","text":"<ul> <li> Create Dockerfile</li> <li> Build and push to GHCR</li> <li> Create Kubernetes manifests</li> <li> Deploy to cluster</li> </ul>"},{"location":"chatwoot-notifier-plan/#phase-4-chatwoot-configuration","title":"Phase 4: Chatwoot Configuration","text":"<ul> <li> Register webhook</li> <li> Test end-to-end flow</li> <li> Monitor and tune</li> </ul>"},{"location":"chatwoot-notifier-plan/#phase-5-enhancements","title":"Phase 5: Enhancements","text":"<ul> <li> Add Slack integration (optional)</li> <li> Add bus notifications for instant popups</li> <li> Add metrics/monitoring</li> <li> Add SLA warning (pre-breach) notifications</li> </ul>"},{"location":"chatwoot-notifier-plan/#testing","title":"Testing","text":""},{"location":"chatwoot-notifier-plan/#local-development","title":"Local Development","text":"<pre><code># Run locally with ngrok for webhook testing\nngrok http 5000\n\n# Update Chatwoot webhook URL temporarily\n# Test by creating a conversation in Chatwoot\n</code></pre>"},{"location":"chatwoot-notifier-plan/#cluster-testing","title":"Cluster Testing","text":"<pre><code># Check logs\nkubectl logs -n business-system -l app.kubernetes.io/name=chatwoot-notifier -f\n\n# Test webhook manually\nkubectl exec -n business-system chatwoot-web-xxx -- curl -X POST \\\n  http://chatwoot-notifier.business-system.svc.cluster.local:5000/webhook/chatwoot \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"event\": \"conversation_updated\", \"id\": 1, \"account\": {\"id\": 1}}'\n</code></pre>"},{"location":"chatwoot-notifier-plan/#references","title":"References","text":"<ul> <li>Chatwoot Webhook Events</li> <li>Chatwoot API Docs</li> <li>Odoo External API</li> <li>Odoo mail.activity</li> </ul>"},{"location":"gotchas/","title":"Gotchas","text":"<p>Unique issues we ran into forking and setting up this repo.</p>"},{"location":"gotchas/#kustomize-mutating-webhook-caches-substitution-values","title":"Kustomize Mutating Webhook Caches Substitution Values","text":"<p>Symptom: Updated a secret (e.g., <code>CLOUDFLARED_TUNNEL_ID</code> in <code>cluster-secrets</code>), but Kustomizations still use the old value.</p> <p>Cause: The <code>kustomize-mutating-webhook</code> bakes substitution values into <code>postBuild.substitute</code> at Kustomization creation time. Updating the source Secret doesn't automatically update existing Kustomizations.</p> <p>Fix: <pre><code># 1. Restart the webhook\nkubectl rollout restart deployment/kustomize-mutating-webhook -n flux-system\n\n# 2. Delete the affected Kustomization\nkubectl delete kustomization &lt;name&gt; -n &lt;namespace&gt;\n\n# 3. Trigger parent to recreate it\nkubectl annotate kustomization cluster -n flux-system reconcile.fluxcd.io/requestedAt=\"$(date +%s)\" --overwrite\n</code></pre></p>"},{"location":"odoo-filestore-backup/","title":"Odoo Filestore Backup Implementation Guide","text":"<p>This guide sets up VolSync with restic to backup the <code>odoo-filestore</code> PVC to GCS.</p>"},{"location":"odoo-filestore-backup/#architecture","title":"Architecture","text":"<pre><code>odoo-filestore PVC (Ceph RBD)\n        \u2502\n        \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   VolSync        \u2502\n\u2502 ReplicationSource\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502 restic + rclone\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  GCS Bucket      \u2502\n\u2502 hayden-odoo-     \u2502\n\u2502 backups/filestore\u2502\n\u2502 (CMEK encrypted) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"odoo-filestore-backup/#encryption","title":"Encryption","text":"<p>Both PostgreSQL and filestore backups are protected:</p> Backup Client-side Server-side (CMEK) PostgreSQL (Barman) - GCP KMS <code>backup-encryption</code> key Filestore (Restic) Restic password GCP KMS <code>backup-encryption</code> key <p>Filestore is double-encrypted: restic encrypts before upload, then GCS CMEK encrypts at rest.</p>"},{"location":"odoo-filestore-backup/#prerequisites","title":"Prerequisites","text":"<ul> <li> GCS bucket exists (<code>hayden-odoo-backups</code>) with CMEK encryption</li> <li> Service account with storage access (<code>odoo-pg-backup</code>)</li> <li> VolSync deployed (ks.yaml added to cluster overlay)</li> <li> ReplicationSource configured</li> <li> Encrypt the restic secret with SOPS (see below)</li> </ul>"},{"location":"odoo-filestore-backup/#sops-secret-setup-required","title":"SOPS Secret Setup (Required)","text":"<p>The restic credentials are stored in a SOPS-encrypted secret (GitOps pattern), not 1Password. This ensures backup recovery only needs: git repo + SOPS decryption key</p>"},{"location":"odoo-filestore-backup/#edit-and-encrypt-the-secret","title":"Edit and encrypt the secret:","text":"<pre><code># 1. Generate a restic password\nRESTIC_PW=$(openssl rand -base64 32)\necho \"RESTIC_PASSWORD: $RESTIC_PW\"\n\n# 2. Get GCS service account JSON (choose one method):\n# From 1Password:\n#   Copy \"serviceAccount\" field from \"odoo-objstore\" item\n# From Terraform:\nterraform -chdir=terraform/gcp output -raw odoo_backup_sa_key | base64 -d &gt; /tmp/gcs-sa.json\ncat /tmp/gcs-sa.json\n\n# 3. Edit the secret with SOPS (will auto-encrypt on save):\nsops kubernetes/apps/base/business-system/odoo/app/secret-filestore-restic.enc.yaml\n\n# 4. Replace the placeholder values:\n#    - RESTIC_PASSWORD: paste the generated password\n#    - RCLONE_CONFIG_GCS_SERVICE_ACCOUNT_CREDENTIALS: paste the full JSON\n</code></pre>"},{"location":"odoo-filestore-backup/#verify-encryption","title":"Verify encryption:","text":"<pre><code># Should show ENC[AES256_GCM,...] for sensitive values\ncat kubernetes/apps/base/business-system/odoo/app/secret-filestore-restic.enc.yaml\n</code></pre>"},{"location":"odoo-filestore-backup/#implementation-details","title":"Implementation Details","text":""},{"location":"odoo-filestore-backup/#sops-secret","title":"SOPS Secret","text":"<p>Located at <code>kubernetes/apps/base/business-system/odoo/app/secret-filestore-restic.enc.yaml</code>.</p> <p>Uses rclone backend for GCS (avoids credential file issues): - <code>RESTIC_REPOSITORY</code>: <code>rclone:gcs:hayden-odoo-backups/filestore</code> - <code>RESTIC_PASSWORD</code>: Encrypted with SOPS - <code>RCLONE_CONFIG_GCS_*</code>: GCS config via environment variables</p>"},{"location":"odoo-filestore-backup/#replicationsource","title":"ReplicationSource","text":"<p>Located at <code>kubernetes/apps/base/business-system/odoo/app/replicationsource.yaml</code>.</p> <ul> <li>Schedule: Daily at 4:00 UTC (15:00 AEDT)</li> <li>Retention: 7 daily, 4 weekly, 3 monthly snapshots</li> <li>Method: Ceph CSI snapshots for consistency</li> <li>Cache: 2Gi for faster incremental backups</li> </ul>"},{"location":"odoo-filestore-backup/#deploy-and-verify","title":"Deploy and Verify","text":"<ol> <li> <p>Commit and push changes <pre><code>git add -A\ngit commit -m \"feat(odoo): add filestore backup with VolSync\"\ngit push\n</code></pre></p> </li> <li> <p>Wait for Flux reconciliation <pre><code>flux reconcile kustomization flux-system --with-source\nkubectl get kustomization -n business-system odoo\n</code></pre></p> </li> <li> <p>Verify VolSync deployment <pre><code>kubectl get pods -n volsync-system\nkubectl get replicationsource -n business-system\n</code></pre></p> </li> <li> <p>Check initial sync status <pre><code>kubectl describe replicationsource odoo-filestore -n business-system\n</code></pre></p> </li> <li> <p>Trigger manual sync (optional) <pre><code>kubectl annotate replicationsource odoo-filestore -n business-system \\\n  volsync.backube/trigger=\"$(date +%s)\" --overwrite\n</code></pre></p> </li> </ol>"},{"location":"odoo-filestore-backup/#monitoring","title":"Monitoring","text":""},{"location":"odoo-filestore-backup/#check-backup-status","title":"Check backup status","text":"<pre><code>kubectl get replicationsource -n business-system -o wide\n</code></pre>"},{"location":"odoo-filestore-backup/#view-backup-logs","title":"View backup logs","text":"<pre><code>kubectl logs -n business-system -l volsync.backube/source=odoo-filestore -f\n</code></pre>"},{"location":"odoo-filestore-backup/#list-snapshots-in-gcs","title":"List snapshots in GCS","text":"<pre><code># Get password from SOPS-encrypted secret\nexport RESTIC_PASSWORD=$(sops -d --extract '[\"stringData\"][\"RESTIC_PASSWORD\"]' \\\n  kubernetes/apps/base/business-system/odoo/app/secret-filestore-restic.enc.yaml)\n\n# List snapshots (requires rclone configured with GCS)\nrestic -r rclone:gcs:hayden-odoo-backups/filestore snapshots\n</code></pre>"},{"location":"odoo-filestore-backup/#restore-procedure","title":"Restore Procedure","text":"<p>To restore the filestore:</p> <ol> <li> <p>Create ReplicationDestination <pre><code>apiVersion: volsync.backube/v1alpha1\nkind: ReplicationDestination\nmetadata:\n  name: odoo-filestore-restore\n  namespace: business-system\nspec:\n  trigger:\n    manual: restore-once\n  restic:\n    repository: odoo-filestore-restic\n    destinationPVC: odoo-filestore\n    copyMethod: Direct\n</code></pre></p> </li> <li> <p>Trigger restore <pre><code>kubectl annotate replicationdestination odoo-filestore-restore -n business-system \\\n  volsync.backube/trigger=\"$(date +%s)\" --overwrite\n</code></pre></p> </li> </ol>"},{"location":"odoo-filestore-backup/#complete-odoo-backup-summary","title":"Complete Odoo Backup Summary","text":"Component Method Destination Schedule PostgreSQL DB CNPG/Barman <code>gs://hayden-odoo-backups/base</code> Weekly (Sunday 3am UTC) PostgreSQL WAL CNPG/Barman <code>gs://hayden-odoo-backups/base</code> Continuous Filestore VolSync/restic <code>gs://hayden-odoo-backups/filestore</code> Daily (4am UTC)"},{"location":"odoo-filestore-backup/#recovery-point-objectives","title":"Recovery Point Objectives","text":"<ul> <li>Database: Point-in-time recovery to any moment (via WAL)</li> <li>Filestore: Daily snapshots (up to 24h data loss possible)</li> </ul>"},{"location":"odoo-filestore-backup/#disaster-recovery-requirements","title":"Disaster Recovery Requirements","text":"<p>To restore from complete loss, you need: 1. Git repository (contains all manifests + SOPS-encrypted secrets) 2. SOPS decryption key (PGP key <code>67AF5B5A73800481D8E41667C87721FBF6BBF30C</code> or age key or GCP KMS access) 3. GCS bucket access (service account credentials are in SOPS secret)</p> <p>All backup credentials are in git - no 1Password dependency for restoration.</p>"},{"location":"velero-odoo-backup/","title":"Velero Odoo Backup Implementation","text":"<p>Tiered backup strategy for Odoo: PITR to GCS (CNPG/Barman) plus daily Velero snapshots to GCS.</p>"},{"location":"velero-odoo-backup/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Odoo Backup Strategy                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502  Tier 1: CNPG/Barman \u2192 GCS (PITR)                               \u2502\n\u2502  \u251c\u2500\u2500 Continuous WAL archiving + base backups                    \u2502\n\u2502  \u251c\u2500\u2500 RPO: ~5 minutes                                            \u2502\n\u2502  \u251c\u2500\u2500 Use case: Point-in-time recovery                           \u2502\n\u2502  \u2514\u2500\u2500 Destination: gs://hayden-odoo-backups/base                 \u2502\n\u2502                                                                 \u2502\n\u2502  Tier 2: Velero \u2192 GCS (offsite)                                 \u2502\n\u2502  \u251c\u2500\u2500 Daily snapshots (PostgreSQL PVC + filestore PVC)           \u2502\n\u2502  \u251c\u2500\u2500 RPO: 24 hours                                              \u2502\n\u2502  \u251c\u2500\u2500 Use case: Disaster recovery if site is lost                \u2502\n\u2502  \u2514\u2500\u2500 Destination: gs://hayden-velero-backups                    \u2502\n\u2502                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"velero-odoo-backup/#prerequisites","title":"Prerequisites","text":"<ul> <li> GCS bucket: <code>hayden-velero-backups</code> (terraform/gcp/velero.tf)</li> <li> Service account with Storage Object Admin</li> <li> Ceph CSI VolumeSnapshotClass</li> <li> Velero SA key in 1Password (<code>velero-gcs</code> item)</li> <li> CNPG backup to GCS running (<code>gs://hayden-odoo-backups/base</code>)</li> <li> VolSync filestore backup to GCS running (<code>gs://hayden-odoo-backups/filestore</code>)</li> <li>Velero adds a namespace-level daily snapshot to a separate bucket</li> </ul>"},{"location":"velero-odoo-backup/#step-1-gcs-credentials","title":"Step 1: GCS Credentials","text":"<p>Velero requires its own SA key (not <code>odoo-objstore</code>) because IAM bindings are bucket-specific.</p> <pre><code>cd terraform/gcp\nterraform output -raw velero_sa_key | base64 -d &gt; /tmp/velero-credentials.json\n\n# Create 1Password item:\n# - Vault: Kubernetes\n# - Item name: velero-gcs\n# - Field name: credentials\n# - Field value: contents of /tmp/velero-credentials.json\n</code></pre>"},{"location":"velero-odoo-backup/#step-2-create-velero-kubernetes-resources","title":"Step 2: Create Velero Kubernetes Resources","text":""},{"location":"velero-odoo-backup/#21-directory-structure","title":"2.1 Directory Structure","text":"<pre><code>kubernetes/apps/base/backup-system/velero/\n\u251c\u2500\u2500 app/\n\u2502   \u251c\u2500\u2500 helmrelease.yaml\n\u2502   \u251c\u2500\u2500 ocirepository.yaml\n\u2502   \u251c\u2500\u2500 externalsecret.yaml\n\u2502   \u2514\u2500\u2500 kustomization.yaml\n\u251c\u2500\u2500 ks.yaml\n\u2514\u2500\u2500 schedules/\n    \u2514\u2500\u2500 odoo-backup.yaml\n</code></pre>"},{"location":"velero-odoo-backup/#22-namespace","title":"2.2 Namespace","text":"<pre><code># kubernetes/apps/base/backup-system/namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: velero\n  labels:\n    pod-security.kubernetes.io/enforce: privileged\n</code></pre>"},{"location":"velero-odoo-backup/#23-external-secret-1password-k8s-secret","title":"2.3 External Secret (1Password \u2192 K8s Secret)","text":"<pre><code># kubernetes/apps/base/backup-system/velero/app/externalsecret.yaml\napiVersion: external-secrets.io/v1\nkind: ExternalSecret\nmetadata:\n  name: velero-gcs\n  namespace: velero\nspec:\n  secretStoreRef:\n    kind: ClusterSecretStore\n    name: onepassword\n  target:\n    name: velero-gcs\n    template:\n      data:\n        cloud: \"{{ .credentials }}\"\n  dataFrom:\n    - extract:\n        key: velero-gcs\n</code></pre>"},{"location":"velero-odoo-backup/#24-helm-release","title":"2.4 Helm Release","text":"<pre><code># kubernetes/apps/base/backup-system/velero/app/helmrelease.yaml\napiVersion: helm.toolkit.fluxcd.io/v2\nkind: HelmRelease\nmetadata:\n  name: velero\n  namespace: velero\nspec:\n  interval: 30m\n  chartRef:\n    kind: OCIRepository\n    name: velero\n  values:\n    image:\n      repository: velero/velero\n      tag: v1.15.0\n\n    initContainers:\n      - name: velero-plugin-for-gcp\n        image: velero/velero-plugin-for-gcp:v1.11.0\n        volumeMounts:\n          - name: plugins\n            mountPath: /target\n\n      - name: velero-plugin-for-csi\n        image: velero/velero-plugin-for-csi:v0.8.0\n        volumeMounts:\n          - name: plugins\n            mountPath: /target\n\n    configuration:\n      backupStorageLocation:\n        - name: gcs\n          provider: gcp\n          bucket: hayden-velero-backups\n          credential:\n            name: velero-gcs\n            key: cloud\n\n      volumeSnapshotLocation:\n        - name: csi-ceph-blockpool\n          provider: velero.io/csi\n\n      defaultBackupStorageLocation: gcs\n      defaultVolumeSnapshotLocations: velero.io/csi:csi-ceph-blockpool\n\n      features: EnableCSI\n\n    credentials:\n      useSecret: true\n      existingSecret: velero-gcs\n\n    snapshotsEnabled: true\n    deployNodeAgent: true\n\n    nodeAgent:\n      podVolumePath: /var/lib/kubelet/pods\n      privileged: true\n\n    schedules: {}  # Managed separately via CRDs\n\n    resources:\n      requests:\n        cpu: 100m\n        memory: 256Mi\n      limits:\n        memory: 512Mi\n</code></pre>"},{"location":"velero-odoo-backup/#25-oci-repository","title":"2.5 OCI Repository","text":"<pre><code># kubernetes/apps/base/backup-system/velero/app/ocirepository.yaml\napiVersion: source.toolkit.fluxcd.io/v1beta2\nkind: OCIRepository\nmetadata:\n  name: velero\n  namespace: velero\nspec:\n  interval: 12h\n  url: oci://ghcr.io/vmware-tanzu/helm-charts/velero\n  ref:\n    tag: 8.2.0\n</code></pre>"},{"location":"velero-odoo-backup/#26-app-kustomization","title":"2.6 App Kustomization","text":"<pre><code># kubernetes/apps/base/backup-system/velero/app/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamespace: velero\nresources:\n  - externalsecret.yaml\n  - ocirepository.yaml\n  - helmrelease.yaml\n</code></pre>"},{"location":"velero-odoo-backup/#step-3-odoo-backup-schedule-with-hooks","title":"Step 3: Odoo Backup Schedule with Hooks","text":""},{"location":"velero-odoo-backup/#31-label-resources-so-selection-works","title":"3.1 Label resources so selection works","text":"<ul> <li>PVCs and CNPG pods currently have no <code>app.kubernetes.io/name: odoo</code> label. Either:</li> <li>Add that label to the PVC and CNPG pod template (recommended), or</li> <li>Remove the labelSelector from the schedule to back up the whole <code>business-system</code> namespace.</li> </ul> <p>Example labels (preferred):</p> <pre><code># kubernetes/apps/base/business-system/odoo/app/pvc.yaml\nmetadata:\n  name: odoo-filestore\n  namespace: business-system\n  labels:\n    app.kubernetes.io/name: odoo\n</code></pre> <pre><code># kubernetes/apps/base/business-system/odoo/app/postgres-cluster.yaml\nspec:\n  instances: 3\n  podTemplate:\n    metadata:\n      labels:\n        app.kubernetes.io/name: odoo\n</code></pre>"},{"location":"velero-odoo-backup/#32-annotate-odoo-pod-for-hooks","title":"3.2 Annotate Odoo pod for hooks","text":"<p>Add to Odoo HelmRelease values:</p> <pre><code># kubernetes/apps/base/business-system/odoo/app/helmrelease.yaml\npodAnnotations:\n  # Pre-backup: Signal PostgreSQL to start backup mode\n  backup.velero.io/backup-volumes: filestore\n  pre.hook.backup.velero.io/container: odoo\n  pre.hook.backup.velero.io/command: '[\"/bin/sh\", \"-c\", \"echo Starting backup\"]'\n  post.hook.backup.velero.io/container: odoo\n  post.hook.backup.velero.io/command: '[\"/bin/sh\", \"-c\", \"echo Backup complete\"]'\n</code></pre>"},{"location":"velero-odoo-backup/#33-cnpg-pod-hooks-not-required","title":"3.3 CNPG Pod Hooks (Not Required)","text":"<p>Note: CNPG pods do NOT need Velero hooks. CNPG/Barman already handles PostgreSQL consistency via WAL archiving (Tier 1). CSI snapshots are crash-consistent, and CNPG can recover from crash-consistent state using WAL replay. Adding <code>pg_backup_start()</code> hooks would be redundant and may fail due to CNPG's internal connection handling.</p> <p>If you still want application-level quiescence for the Odoo pod, only the Odoo deployment needs hooks (covered in 3.2).</p>"},{"location":"velero-odoo-backup/#34-scheduled-backup","title":"3.4 Scheduled Backup","text":"<pre><code># kubernetes/apps/base/backup-system/velero/schedules/odoo-backup.yaml\napiVersion: velero.io/v1\nkind: Schedule\nmetadata:\n  name: odoo-daily\n  namespace: velero\nspec:\n  # Daily at 03:00 UTC (14:00 AEDT)\n  schedule: \"0 3 * * *\"\n  template:\n    # Backup business-system namespace\n    includedNamespaces:\n      - business-system\n\n    # Include specific resources\n    includedResources:\n      - persistentvolumeclaims\n      - persistentvolumes\n      - pods\n      - deployments\n      - statefulsets\n      - configmaps\n      - secrets\n\n    # Label selector for Odoo resources (remove if you do not add labels)\n    labelSelector:\n      matchLabels:\n        app.kubernetes.io/name: odoo\n\n    # Snapshot PVCs\n    snapshotVolumes: true\n    storageLocation: gcs\n    volumeSnapshotLocations:\n      - csi-ceph-blockpool\n\n    # Retention\n    ttl: 168h  # 7 days\n\n    # Hooks enabled\n    hooks:\n      resources: []\n\n  useOwnerReferencesInBackup: false\n</code></pre>"},{"location":"velero-odoo-backup/#step-4-flux-kustomization","title":"Step 4: Flux Kustomization","text":"<pre><code># kubernetes/apps/base/backup-system/velero/ks.yaml\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: velero\n  namespace: flux-system\nspec:\n  targetNamespace: velero\n  path: ./apps/base/backup-system/velero/app\n  sourceRef:\n    kind: GitRepository\n    name: flux-system\n  interval: 30m\n  prune: true\n  wait: true\n  dependsOn:\n    - name: external-secrets\n    - name: rook-ceph-cluster\n  decryption:\n    provider: sops\n    secretRef:\n      name: sops-gpg\n</code></pre>"},{"location":"velero-odoo-backup/#step-5-manual-backup-test","title":"Step 5: Manual Backup Test","text":""},{"location":"velero-odoo-backup/#install-velero-cli-if-not-installed","title":"Install Velero CLI (if not installed)","text":"<pre><code># macOS\nbrew install velero\n\n# Linux\ncurl -LO https://github.com/vmware-tanzu/velero/releases/download/v1.15.0/velero-v1.15.0-linux-amd64.tar.gz\ntar -xzf velero-v1.15.0-linux-amd64.tar.gz\nsudo mv velero-v1.15.0-linux-amd64/velero /usr/local/bin/\n</code></pre>"},{"location":"velero-odoo-backup/#run-test-backup","title":"Run Test Backup","text":"<pre><code># Create on-demand backup\nvelero backup create odoo-test-$(date +%Y%m%d-%H%M) \\\n  --include-namespaces business-system \\\n  --snapshot-volumes \\\n  --wait\n\n# Check backup status\nvelero backup describe odoo-test-XXXXXX --details\n\n# List backups\nvelero backup get\n</code></pre>"},{"location":"velero-odoo-backup/#step-6-restore-procedure","title":"Step 6: Restore Procedure","text":"<pre><code># List available backups\nvelero backup get\n\n# Restore to same namespace\nvelero restore create --from-backup odoo-daily-XXXXXX\n\n# Restore to different namespace (DR)\nvelero restore create --from-backup odoo-daily-XXXXXX \\\n  --namespace-mappings business-system:business-system-restore\n</code></pre>"},{"location":"velero-odoo-backup/#backup-strategy-summary","title":"Backup Strategy Summary","text":"Tier Component Method Destination Schedule Retention RPO 1 PostgreSQL CNPG WAL + base backup <code>gs://hayden-odoo-backups/base</code> WAL continuous, weekly base 7 days ~5 min 2 Namespace (DB + filestore PVCs) Velero CSI snapshot <code>gs://hayden-velero-backups</code> Daily 03:00 UTC 7 days 24h 2 (existing) Filestore PVC VolSync/restic <code>gs://hayden-odoo-backups/filestore</code> Daily 04:00 UTC 7/4/3 (d/w/m) 24h"},{"location":"velero-odoo-backup/#notes","title":"Notes","text":"<ul> <li>TrueNAS/MinIO tier is deferred; keep CNPG pointed at GCS until hardware is ready.</li> <li>Flux wiring: add <code>velero</code> to <code>kubernetes/apps/overlays/cluster-00/kustomization.yaml</code> resources to deploy.</li> <li>The Schedule's <code>hooks.resources: []</code> is intentional - pod-level hooks are configured via annotations (section 3.2).</li> </ul>"},{"location":"velero-odoo-backup/#implementation-status","title":"Implementation Status","text":"<ul> <li> Create 1Password item <code>velero-gcs</code> with Velero SA key (terraform output)</li> <li> Create <code>kubernetes/apps/base/backup-system/</code> directory structure</li> <li> Create namespace.yaml</li> <li> Create velero app resources (externalsecret, ocirepository, helmrelease, kustomization)</li> <li> Create velero ks.yaml (Flux Kustomization)</li> <li> Create odoo-backup schedule</li> <li> Add labels to Odoo PVC</li> <li> Add Velero annotations to Odoo HelmRelease</li> <li> Wire up in cluster-00 overlay</li> <li> Commit and deploy</li> <li> Test manual backup</li> <li> Verify scheduled backup runs</li> </ul>"},{"location":"velero-odoo-backup/#troubleshooting","title":"Troubleshooting","text":"<pre><code># Check Velero logs\nkubectl logs -n velero deployment/velero\n\n# Check backup logs\nvelero backup logs odoo-daily-XXXXXX\n\n# Check node-agent (for file-level backup)\nkubectl logs -n velero daemonset/node-agent\n\n# Verify VolumeSnapshotClass\nkubectl get volumesnapshotclass\n\n# Check CSI snapshots\nkubectl get volumesnapshot -n business-system\n</code></pre>"},{"location":"velero-odoo-backup/#references","title":"References","text":"<ul> <li>Velero GCP Plugin</li> <li>Velero Backup Hooks</li> <li>CloudNativePG Backup</li> </ul>"},{"location":"configuration/api-access/","title":"Access Kubernetes API through Cloudflare","text":""},{"location":"configuration/api-access/#prerequisites","title":"Prerequisites","text":"<p>Before proceeding, make sure you have the following prerequisites in place to set up your kubeconfig locally.</p>"},{"location":"configuration/api-access/#setup-kubeconfig-locally","title":"Setup Kubeconfig Locally","text":"<p>To configure the kubeconfig correctly, follow the steps below:</p> <ol> <li> <p>Go to https://login.haydenagencies.com.au in your web browser.</p> </li> <li> <p>Once authenticated, the webpage will provide command prompts specific to your account. These commands need to be set up on the client machine that you will be using with kubectl to run commands.</p> </li> </ol> <p>Note: The commands provided will configure the necessary authentication and context for your kubeconfig, enabling seamless interaction with the Kubernetes cluster.</p> <p>By following these steps, you will set up the kubeconfig correctly on your local machine, allowing you to access and manage the Kubernetes cluster using kubectl commands.</p>"},{"location":"configuration/api-access/#connect-from-a-client-machine","title":"Connect From a Client Machine","text":"<p>To enable remote kubectl access to the Kubernetes cluster, follow the instructions below.</p> <p>Note: These instructions are applicable if you are part of the Raspbernetes project and have been granted access by an admin.</p>"},{"location":"configuration/api-access/#1-install-cloudflared-on-the-client-machine","title":"1. Install Cloudflared on the Client Machine","text":"<p>Download and install cloudflared on the client machine that will connect to the Kubernetes cluster. You can find the installation instructions HERE.</p> <p><code>Cloudflared</code> will need to be installed on each user device that will connect to the kube-apiserver.</p>"},{"location":"configuration/api-access/#2-establish-connection","title":"2. Establish Connection","text":"<p>Run the following command to create a connection from the device to Cloudflare. Any available port can be specified.</p> <pre><code>$ cloudflared access tcp --hostname api.haydenagencies.com.au --url 127.0.0.1:1234\n</code></pre> <p>With this service running, you can run a <code>kubectl</code> command and <code>cloudflared</code> will launch a browser window and prompt the user to authenticate with the Github SSO provider. Once authenticated, <code>cloudflared</code> will expose the connection to the client machine at the local URL specified in the command.</p> <p><code>kubeconfig</code> does not support proxy command configurations at this time, though the community has submitted plans to do so. In the interim, users can alias the cluster's API server to save time.</p> <pre><code>$ alias kubeone=\"env HTTPS_PROXY=socks5://127.0.0.1:1234 kubectl\"\n</code></pre>"},{"location":"configuration/api-access/#3-test-connection","title":"3. Test Connection","text":"<p>To test the connection, use the alias and run a kubectl command. For example:</p> <pre><code>kubeone get nodes\n</code></pre> <p>If the connection is successful, you should see the appropriate information about the cluster's nodes.</p> <p>Example result:</p> <pre><code>NAME            STATUS   ROLES            AGE   VERSION\nk8s-master-01   Ready    control-plane    8h    v1.26.1\nk8s-master-02   Ready    control-plane    8h    v1.26.1\nk8s-master-03   Ready    control-plane    8h    v1.26.1\nk8s-worker-01   Ready    &lt;none&gt;           8h    v1.26.1\n</code></pre> <p>You now have complete access to the cluster using the set alias. Please ensure that the cluster has RBAC enabled and that you have been granted the necessary user permissions by an admin.</p> <p>Note: Official documentation can also be referenced HERE</p>"},{"location":"configuration/ip-allocation/","title":"IP Allocation","text":"<p>The IP Allocation section provides an overview of the allocated IP addresses for different applications, nodes, and external devices within my environment.</p>"},{"location":"configuration/ip-allocation/#kubernetes-nodes","title":"Kubernetes Node(s)","text":"<p>The Kubernetes Node(s) table displays the IP addresses assigned to each node in the cluster:</p> Node/Instance Type IP/CIDR Control Plane VIP 192.168.50.200/32 Protectli FW2B 01 Control Plane 192.168.50.111/32 Protectli FW2B 02 Control Plane 192.168.50.112/32 Protectli FW2B 03 Control Plane 192.168.50.113/32 Protectli VP2410 01 Node 192.168.50.114/32 Protectli VP2410 02 Node 192.168.50.115/32 Protectli VP2410 03 Node 192.168.50.116/32 Raspberry Pi 01 Node 192.168.50.121/32 Raspberry Pi 02 Node 192.168.50.122/32 Raspberry Pi 03 Node 192.168.50.123/32 Raspberry Pi 04 Node 192.168.50.124/32 Rock Pi 01 Node 192.168.50.131/32 Rock Pi 02 Node 192.168.50.132/32 Rock Pi 03 Node 192.168.50.133/32"},{"location":"configuration/ip-allocation/#kubernetes-applications","title":"Kubernetes Application(s)","text":"<p>The Kubernetes Application(s) table presents the IP addresses allocated to different applications in the Kubernetes cluster:</p> Application/Node Type IP/CIDR Metallb Daemonset 192.168.50.150 &lt;-&gt; 192.168.50.155 Istio Ingress LoadBalancer 192.168.50.180/32 Coredns LoadBalancer 192.168.50.181/32 Mosquitto LoadBalancer 192.168.50.182/32 Zigbee2mqtt LoadBalancer 192.168.50.183/32 Nginx Ingress LoadBalancer 192.168.50.180/32 K8s Gateway LoadBalancer 192.168.50.188/32 Blocky LoadBalancer 192.168.50.191/32"},{"location":"configuration/ip-allocation/#external-devices","title":"External Device(s)","text":"<p>The External Device(s) table lists IP addresses assigned to devices outside the Kubernetes cluster:</p> Application Type IP/CIDR Zigbee Controller N/A 192.168.50.165/32 Ender 5 Pro 3D Printer N/A x.x.x.x/32"},{"location":"configuration/repo-structure/","title":"Flux Repository Structure","text":"<p>Work in progress</p> <p>This document is a work in progress.</p>"},{"location":"configuration/repo-structure/#tldr-quick-start","title":"TL;DR Quick Start","text":"<p>If you're familiar with Kustomize and how it operates within the Flux ecosystem this will provide a quick overview:</p> <pre><code>.\n\u2514\u2500\u2500 kubernetes/\n    \u251c\u2500\u2500 clusters/\n    \u2502   \u251c\u2500\u2500 production/                         # One folder per cluster.\n    \u2502   \u2502   \u251c\u2500\u2500 flux-system/                    # Folder containing flux-system manifests.\n    \u2502   \u2502   \u2502   \u251c\u2500\u2500 ...                         # Flux component resource manifests.\n    \u2502   \u2502   \u2502   \u2514\u2500\u2500 kustomization.yaml          # Generated kustomization per cluster bootstrap.\n    \u2502   \u2502   \u2514\u2500\u2500 kustomization.yaml              # Kustomization per cluster referring all manifests in core and namespace directory.\n    \u2502   \u2514\u2500\u2500 staging/\n    \u2502       \u251c\u2500\u2500 flux-system/\n    \u2502       \u2502   \u251c\u2500\u2500 ...\n    \u2502       \u2502   \u2514\u2500\u2500 kustomization.yaml\n    \u2502       \u2514\u2500\u2500 kustomization.yaml\n    \u251c\u2500\u2500 core/\n    \u2502   \u251c\u2500\u2500 base/\n    \u2502   \u2502   \u2514\u2500\u2500 .../                            # One folder per resource type and/or app with its core dependency with prune disabled.\n    \u2502   \u2502       \u2514\u2500\u2500 application/                # One folder per application with core manifests.\n    \u2502   \u2502           \u2514\u2500\u2500 kustomization.yaml      # Kustomization per core application.\n    \u2502   \u2514\u2500\u2500 overlays/\n    \u2502       \u251c\u2500\u2500 production/\n    \u2502       \u2502   \u251c\u2500\u2500 kustomization.yaml          # Kustomization per cluster referencing each core app required.\n    \u2502       \u2502   \u2514\u2500\u2500 patch.yaml                  # Optional patch for each environment.\n    \u2502       \u2514\u2500\u2500 staging/\n    \u2502           \u251c\u2500\u2500 kustomization.yaml\n    \u2502           \u2514\u2500\u2500 patch.yaml\n    \u2514\u2500\u2500 namespaces/\n        \u251c\u2500\u2500 base/\n        \u2502   \u2514\u2500\u2500 namespace/                      # One folder per namespace containing base resources.\n        \u2502       \u251c\u2500\u2500 namespace.yaml              # Namespace manifest.\n        \u2502       \u251c\u2500\u2500 kustomization.yaml          # Kustomization per namespace referring all manifests in this current directory.\n        \u2502       \u2514\u2500\u2500 application/                # Folder per app containing manifests and patches for each application.\n        \u2502           \u2514\u2500\u2500 kustomizaiton.yaml      # Kustomization per app referring all manifests in this directory.\n        \u2514\u2500\u2500 overlays/\n            \u251c\u2500\u2500 production/\n            \u2502   \u251c\u2500\u2500 kustomization.yaml          # Kustomization per cluster referencing each namespace and app required.\n            \u2502   \u2514\u2500\u2500 patch.yaml                  # Optional patch for each environment.\n            \u2514\u2500\u2500 staging/\n                \u251c\u2500\u2500 kustomization.yaml\n                \u2514\u2500\u2500 patch.yaml\n</code></pre>"},{"location":"configuration/repo-structure/#repository-structure-breakdown","title":"Repository Structure Breakdown","text":"<p>This Git repository contains the following directories:</p> <ul> <li>clusters dir contains the Flux configuration per cluster.</li> <li>core dir contains cluster resources that are core prerequisites to the cluster.</li> <li>namespaces dir contains namespaces and application workloads per cluster.</li> </ul> <pre><code>.\n\u251c\u2500\u2500 clusters/\n\u2502   \u251c\u2500\u2500 production\n\u2502   \u2514\u2500\u2500 staging\n\u251c\u2500\u2500 core/\n\u2502   \u251c\u2500\u2500 base\n\u2502   \u2514\u2500\u2500 overlays/\n\u2502       \u251c\u2500\u2500 production\n\u2502       \u2514\u2500\u2500 staging\n\u2514\u2500\u2500 namespaces/\n    \u251c\u2500\u2500 base\n    \u2514\u2500\u2500 overlays/\n        \u251c\u2500\u2500 production\n        \u2514\u2500\u2500 staging\n</code></pre> <p>The clusters/ dir contains configuration for each cluster definition and the infrastructure as code for each relevant cluster where applicable.</p> <p>The core/ dir contains all resources that are prerequisites to namespaces and workloads, this includes resources: CRDs, certain applications like Istio and Gatekeeper that must exist prior to other workloads, and crossplane resources that provisions infrastructure.</p> <p>The namespaces/ configuration is structured into:</p> <ul> <li>namespaces/base/ dir contains namespaces and application workload resources.</li> <li>namespaces/overlays/production/ dir contains the production cluster values and references what base components to deploy.</li> <li>namespaces/overlays/staging/ dir contains the stating cluster values and references what base components to deploy.</li> </ul> <pre><code>.\n\u2514\u2500\u2500 namespaces/\n    \u251c\u2500\u2500 base/\n    \u2502   \u2514\u2500\u2500 namespace/\n    \u2502       \u251c\u2500\u2500 namespace.yaml\n    \u2502       \u251c\u2500\u2500 kustomization.yaml\n    \u2502       \u2514\u2500\u2500 application/\n    \u2502           \u251c\u2500\u2500 helmrelease.yaml\n    \u2502           \u2514\u2500\u2500 kustomizaiton.yaml\n    \u2514\u2500\u2500 overlays/\n        \u251c\u2500\u2500 production/\n        \u2502   \u251c\u2500\u2500 kustomization.yaml\n        \u2502   \u2514\u2500\u2500 patch.yaml\n        \u2514\u2500\u2500 staging/\n            \u2514\u2500\u2500 ...\n</code></pre> <p>In namespaces/base/ dir will be a hierarchy of all namespace/ dirs which will contain application resources. Each cluster overlay includes each namespace and/or application which is explicitly referenced; The base application configuration is defined with the following values:</p> <pre><code>apiVersion: helm.toolkit.fluxcd.io/v2beta2\nkind: HelmRelease\nmetadata:\n  name: metallb\n  namespace: network-system\nspec:\n  interval: 5m\n  chart:\n    spec:\n      chart: metallb\n      version: 2.0.4\n      sourceRef:\n        kind: HelmRepository\n        name: bitnami-charts\n        namespace: flux-system\n      interval: 10m\n  values:\n    configInline:\n      address-pools:\n        - name: default\n          protocol: layer2\n          addresses:\n            - 192.168.1.150-192.168.1.155\n</code></pre> <p>In namespaces/overlays/production/ dir we have a Kustomize patch file(s) with the production cluster specific values:</p> <pre><code>apiVersion: helm.toolkit.fluxcd.io/v2beta2\nkind: HelmRelease\nmetadata:\n  name: metallb\n  namespace: network-system\nspec:\n  values:\n    configInline:\n      address-pools:\n        - name: default\n          protocol: layer2\n          addresses:\n            - 192.168.1.150-192.168.1.155\n</code></pre> <p>Note that whilst using Kustomize we can overwrite default values; in this example the default MetalLB address pool will be patched in the production cluster to a unique pool.</p>"},{"location":"configuration/sealed-secrets/","title":"Sealed Secrets","text":"<p>Work in progress</p> <p>This document is a work in progress.</p> <p>When bootstrapping your cluster for the first time you must store a unique public &amp; private key for your sealed-secrets controller to use for managing your sensitive keys and passwords in your Kubernetes cluster.</p>"},{"location":"configuration/sealed-secrets/#install-the-cli-tool","title":"Install the CLI tool","text":"<p>For all installation methods visit the Sealed Secrets install guide</p> <pre><code>brew install kubeseal\n</code></pre>"},{"location":"configuration/sealed-secrets/#remove-existing-key-cert","title":"Remove Existing Key &amp; Cert","text":"<p>Remove the sealed-secrets master key currently present in this repository</p> <pre><code>rm -f kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-private-key.enc.yaml\n</code></pre> <p>Remove the sealed-secrets public cert currently present in this repository</p> <pre><code>rm -f kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-public-cert.yaml\n</code></pre>"},{"location":"configuration/sealed-secrets/#store-new-key-cert","title":"Store New Key &amp; Cert","text":""},{"location":"configuration/sealed-secrets/#private-key","title":"Private Key","text":"<p>Once sealed-secrets has been re-deployed to a running cluster you must store the private key and public cert in this repository.</p> <p>Get the generated sealed-secret private key</p> <pre><code>kubectl get secret -n kube-system -l sealedsecrets.bitnami.com/sealed-secrets-key -o yaml &gt; kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-private-key.yaml\n</code></pre> <p>Encrypt the sealed-secret private key using SOPs</p> <pre><code>sops --encrypt kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-private-key.yaml &gt; kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-private-key.enc.yaml\n</code></pre> <p>Remove unencrypted private key</p> <pre><code>rm -f kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-private-key.yaml\n</code></pre>"},{"location":"configuration/sealed-secrets/#public-cert","title":"Public Cert","text":"<p>Fetch the generated sealed-secret public cert and store it</p> <pre><code>kubeseal \\\n    --controller-name sealed-secrets \\\n    --fetch-cert &gt; kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-public-cert.yaml\n</code></pre>"},{"location":"configuration/sealed-secrets/#encrypt-secrets","title":"Encrypt Secrets","text":"<p>With a newly generated private key from sealed-secrets you will need to re-encrypt all of the existing required secrets.</p> <p>Create an alias for the CLI tool (recommended)</p> <pre><code>alias kubeseal='kubeseal --cert kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-public-cert.pem --controller-name sealed-secrets --format yaml'\n</code></pre> <p>Encrypt new Kubernetes secret</p> <pre><code>kubeseal &lt; secret.yaml &gt; secret.encrypted.yaml\n</code></pre> <p>Remove the unencrypted secret</p> <p>You must encrypt your secrets with the correct cluster public certificate. For more in-depth instructions the official docs can be found here</p>"},{"location":"configuration/sealed-secrets/#offline-decryption","title":"Offline Decryption","text":"<p>Storing the private key allows an offline decryption, this is not recommended and should only be used in a break-glass scenario when the cluster is down and secrets must be accessed.</p> <p>Unencrypt the sealed-secret private key</p> <pre><code>sops --decrypt kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-private-key.enc.yaml -oyaml &gt; kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-private-key.yaml\n</code></pre> <p>Unseal the encrypted secret(s)</p> <pre><code>kubeseal --recovery-unseal --recovery-private-key ./kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-private-key.yaml &lt; &lt;path-to-file&gt;/secret.encrypted.yaml\n</code></pre> <p>Re-Encrypt the sealed-secret private key</p> <pre><code>sops --encrypt kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-private-key.yaml &gt; kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-private-key.enc.yaml\n</code></pre> <p>Remove the unencrypted private key</p> <pre><code>rm -f kubernetes/clusters/&lt;cluster&gt;/secrets/sealed-secret-private-key.yaml\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>We warmly welcome and appreciate all contributions to this project! Whether you're a developer, designer, tester, or documentation enthusiast, your skills and ideas are valuable to us. There is the official contributing guide which can be found HERE.</p>"},{"location":"contributing/#contributors","title":"Contributors","text":"<p>This project owes its success to the incredible individuals who contribute their time, expertise, and passion.</p> <p> </p> <p>Every line of code, bug report, documentation improvement, or idea shared has played a part in shaping this project. We deeply appreciate the dedication and effort of our contributors.</p> <p>Join us on this exciting journey, and let's create something remarkable together! Don't hesitate to get involved and make your mark.</p> <p>Watch this space for the upcoming official contribution guide, which will provide more details on how you can contribute to this project.</p> <p>Thank you for being a part of our vibrant community!</p>"},{"location":"contributing/git-workflow/","title":"Git Workflow (Fork)","text":"<p>Simple steps to keep your fork in sync with the upstream repo and to work on changes safely.</p>"},{"location":"contributing/git-workflow/#daily-sync","title":"Daily sync","text":"<ul> <li>Ensure remotes: <code>origin</code> (your fork), <code>upstream</code> (source). Verify with <code>git remote -v</code>.</li> <li>Update your local <code>master</code> from upstream\u2019s <code>main</code>: <code>task git:sync</code> (or run the commands listed in the Task below manually).</li> <li>If conflicts appear during the rebase, resolve, <code>git add</code> the files, and continue with <code>git rebase --continue</code>. If you want to bail out, run <code>git rebase --abort</code>.</li> </ul>"},{"location":"contributing/git-workflow/#feature-work","title":"Feature work","text":"<ul> <li>Branch off the refreshed <code>master</code>: <code>git checkout -b feature/&lt;short-name&gt;</code>.</li> <li>Commit normally. Rebase the feature branch on <code>master</code> before pushing or opening a PR: <code>git fetch upstream &amp;&amp; git checkout master &amp;&amp; git rebase upstream/main &amp;&amp; git checkout feature/&lt;short-name&gt; &amp;&amp; git rebase master</code>.</li> <li>Push feature branches to your fork: <code>git push --force-with-lease origin feature/&lt;short-name&gt;</code> after rebasing.</li> <li>Open PRs from feature branches. Avoid committing directly to <code>master</code>.</li> </ul>"},{"location":"contributing/git-workflow/#quick-commands","title":"Quick commands","text":"<ul> <li>Sync fork: <code>task git:sync</code></li> <li>Start work: <code>git checkout master &amp;&amp; git checkout -b feature/&lt;short-name&gt;</code></li> <li>Refresh feature: <code>git checkout master &amp;&amp; task git:sync &amp;&amp; git checkout feature/&lt;short-name&gt; &amp;&amp; git rebase master</code></li> </ul>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#fluxcd-vs-argocd","title":"FluxCD vs. ArgoCD","text":"<p>FluxCD and ArgoCD are both popular GitOps tools used for managing and deploying applications in Kubernetes clusters. However, there are some key differences between the two.</p> <p>FluxCD, as a GitOps tool, is built with multi-tenancy, multi-cluster, and multi-cloud environments in mind from the ground up. It provides robust support for managing applications across different tenants, clusters, and cloud platforms, making it highly flexible and scalable. FluxCD offers a rich set of features and capabilities that cater to complex enterprise scenarios.</p> <p>It's important to note that both FluxCD and ArgoCD have their strengths and suitability for different use cases. The choice between the two ultimately depends on the specific requirements and preferences of the organization or project at hand.</p>"},{"location":"installation/","title":"Getting Started","text":"<p>These instructions will assume you have an Kubernetes cluster and want to bootstrap this GitOps repository to it.</p>"},{"location":"installation/#install-the-cli-tool","title":"Install the CLI tool","text":"<p>For all installation methods visit the Flux install guide</p> <pre><code>brew install fluxcd/tap/flux\n</code></pre>"},{"location":"installation/#install-flux","title":"Install Flux","text":"<p>For the full installation guide visit the Flux bootstrap guide</p> <p>Validate the cluster and its connectivity</p> <pre><code>kubectl cluster-info\n</code></pre> <p>Export your GitHub personal access token, username, repository and cluster</p> <pre><code>export GITHUB_TOKEN=&lt;your-token&gt;\nexport GITHUB_USER=&lt;your-username&gt;\nexport GITHUB_REPO=&lt;your-repo&gt;\nexport CLUSTER=&lt;target-cluster&gt;\n</code></pre> <p>Verify that your cluster satisfies the prerequisites</p> <pre><code>flux check --pre\n</code></pre> <p>Run the bootstrap command to install Flux</p> <pre><code>flux bootstrap github \\\n  --owner=\"${GITHUB_USER}\" \\\n  --repository=\"${GITHUB_REPO}\" \\\n  --path=kubernetes/clusters/\"${CLUSTER}\" \\\n  --branch=main \\\n  --personal\n</code></pre> <p>Note: If you have network issues with Flux starting you may need to set <code>--network-policies=false</code> in the bootstrap command.</p> <p>You may also use the automated installation script - Either override the defaults in the install script or as environment variables.</p>"},{"location":"sponsor/","title":"Sponsors","text":"<p>If you have found value in any of my open-source projects, I would greatly appreciate your support as a sponsor. Don't forget to star the repository as well!</p> <p>To become a sponsor, please visit my official GitHub sponsor page HERE.</p>"},{"location":"sponsor/#supporters","title":"Supporters","text":"<p>Thank you to all our backers and sponsors! Your contributions are immensely valuable and greatly appreciated.</p> <p></p> <p> </p>"},{"location":"sponsor/#premium-sponsors","title":"Premium Sponsors","text":"<p>I would like to extend a special thank you to our premium sponsors, whose generous support makes a significant impact on the continued development and success of this project.</p> <ul> <li>Bitscope</li> <li>Rock Pi</li> <li>Protectli</li> <li>Noctua</li> </ul> <p>Your sponsorship plays a crucial role in supporting ongoing enhancements and the sustainability of this project. We are incredibly grateful for your generous contributions!</p> <p>To all our backers and sponsors, your support enables us to maintain and improve this project, benefiting the entire community. Thank you for being a part of our journey!</p>"}]}